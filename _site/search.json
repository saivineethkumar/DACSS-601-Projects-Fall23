[
  {
    "objectID": "challenge_1_Fall23.html",
    "href": "challenge_1_Fall23.html",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "",
    "text": "Make sure you change the author’s name in the above YAML header."
  },
  {
    "objectID": "challenge_1_Fall23.html#setup",
    "href": "challenge_1_Fall23.html#setup",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Setup",
    "text": "Setup\nIf you have not installed the following packages, please install them before loading them.\n\n# To install the packages\n#install.packages(\"tidyverse\")\n#install.packages(\"haven\")\n\n\nlibrary(tidyverse)\nlibrary(haven) #for loading other datafiles (SAS, STATA, SPSS, etc.)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\nlibrary(readxl)\nlibrary(dplyr)"
  },
  {
    "objectID": "challenge_1_Fall23.html#challenge-overview",
    "href": "challenge_1_Fall23.html#challenge-overview",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nThis first weekly challenge aims to practice the following skill sets: 1. Read datasets in different file types; 2. Describe the datasets; 3. Exploring a few basic functions of data transformation and wrangling and present some descriptive statistics (such as min, max, and median).\nThere will be coding components (reading datasets and data transformation) and writing components (describing the datasets and some statistical information). Please read the instructions for each part and complete your challenges."
  },
  {
    "objectID": "challenge_1_Fall23.html#creating-your-own-r-quarto-project-and-make-sure-your-working-directory",
    "href": "challenge_1_Fall23.html#creating-your-own-r-quarto-project-and-make-sure-your-working-directory",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Creating your own R quarto project and make sure your working directory",
    "text": "Creating your own R quarto project and make sure your working directory\n(Will be demonstrated on Sep 20 and 21 lab meetings)"
  },
  {
    "objectID": "challenge_1_Fall23.html#datasets",
    "href": "challenge_1_Fall23.html#datasets",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Datasets",
    "text": "Datasets\nThere are four datasets provided in this challenge. Please download the following dataset files from Canvas or Google Classroom and save them to a folder within your project working directory (i.e.: “yourworkingdiectory_data”). If you don’t have a folder to store the datasets, please create one.\n\nbabynames.csv (Required) ⭐\nESS_5.dta (Option 1) ⭐\np5v2018.sav (Option 2)⭐\nrailroad.xlsx (Required)⭐⭐\n\nFind the _data folder, then use the correct R command to read the datasets."
  },
  {
    "objectID": "challenge_1_Fall23.html#baby-names-dataset-required",
    "href": "challenge_1_Fall23.html#baby-names-dataset-required",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Baby Names Dataset (Required)",
    "text": "Baby Names Dataset (Required)\n\nRead the dataset “babynames.csv”:\n\n\n#Type your code here\n\n\nData Description: Please use the necessary commands and codes and briefly describe this data with a short writing paragraph answering the following questions.\n\n#Type your code here; and write a paragraph answering the questions.\n\n(1) What is the dimension of the data (# of rows and columns)?\n(2) What do the rows and columns mean in this data?\n(3) What is the unit of observation? In other words, what does each case mean in this data?\n(4) According to the lecture, is this a “tidy” data?\nData Transformation: use necessary commands and codes and answer the following questions.\n\n#Type your code here; and write a paragraph answering the questions.\n\n(1) How many unique male names, unique female names, and total unique names are in the data?\n(2) How many years of names does this data record?\n(3) Summarize the min, mean, median, and max of “Occurrence”. (Must use summarize())\n(4) (Optional) Summarize the min, mean, median, and max of “Occurrence” by decade."
  },
  {
    "objectID": "challenge_1_Fall23.html#section",
    "href": "challenge_1_Fall23.html#section",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "",
    "text": "In the following part, choose either one of the two datasets to complete the tasks."
  },
  {
    "objectID": "challenge_1_Fall23.html#optional-1-european-social-survey-dataset",
    "href": "challenge_1_Fall23.html#optional-1-european-social-survey-dataset",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Optional 1: European Social Survey Dataset",
    "text": "Optional 1: European Social Survey Dataset\n\nRead the dataset “ESS_5.dta”.\n\n#Type your code here\n\n\n\n\nData Description: Please use the necessary commands and codes and briefly describe this data with a short writing paragraph answering the following questions.\n(1) What is the dimension of the data (# of rows and columns)?\n\n#Type your code here; and write a paragraph answering the questions.\n\nAs we can see, this data is very large. We don’t want to study the whole data. Let’s just reload the following selected columns: idno, essroud, male, age, edu, income_10, eth_major, media (a standardized measure of the frequency of media consumption), and cntry.\n\n#Type your code here; and write a paragraph answering the questions.\n\n(2) For the reloaded data, what do the rows and columns mean in this data?\n(3) What is the unit of observation? In other words, what does each case mean in this data?\n(4) According to the lecture, is this a “tidy” data?\nData Transformation: use necessary commands and codes and answer the following questions.\n\n#Type your code here; and write a paragraph answering the questions.\n\n(1) How many unique countries are in the data?\n(2) What is the range and average of the following variables: “age”, “edu”, and “media”? Must use summarize().\n(3) How many missing data (NA) are in the following variables: “eth_major” and “income_10”? (tips: use is.na())"
  },
  {
    "objectID": "challenge_1_Fall23.html#optional-2-polity-v-data",
    "href": "challenge_1_Fall23.html#optional-2-polity-v-data",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Optional 2: Polity V Data",
    "text": "Optional 2: Polity V Data\nThe Polity data series is a data series in political science research. Polity is among prominent datasets that measure democracy and autocracy. The Polity5 dataset covers all major, independent states in the global system over the period 1800-2018 (i.e., states with a total population of 500,000 or more in the most recent year; currently 167 countries with Polity5 refinements completed for about half those countries).\n\nRead the dataset “p5v2018.sav”.\n\n#Type your code here\npolityv_data_full &lt;- read_sav(\"./challenge1_data/p5v2018.sav\")\n\n\n\n\nData Description: Please use the necessary commands and codes and briefly describe this data with a short writing paragraph answering the following questions.\n\n#Type your code here; and write a paragraph answering the questions.\ndim(polityv_data_full)\n\n[1] 17574    37\n\nhead(polityv_data_full)\n\n\n\n  \n\n\n\n(1) What is the dimension of the data (# of rows and columns)?\nAnswer: There are 17574 rows and 37 columns in the data.\nAs we can see, this data contains many columns. We don’t want to study the whole data. Let’s keep the first seven columns and the ninth and ten columns.\n\n#Type your code here; and write a paragraph answering the questions.\npolityv_data &lt;- subset(polityv_data_full, select = c(1:7,9, 10) )\n\n(2) For the reloaded data, what do the rows mean in this data? What do the columns (#2-#8) mean? (If you have questions, check out p.11-16 of the User Manual/Codebook of the dataset).\nAnswer: The data has 17574 rows and 9 columns.\nEach row gives information about a country’s democracy and autocracy measures in a particular year.\nThe columns gives information about the country, the year of record, and measures. cyear - country year: a unique identifier for each country year, ccode - a country’s assigned numeric code, scode - a country’s assigned alpha code, country - name of the country, year - year of record, flag - confidence of measures, democ - the democracy index/score, autoc - the autocracy index/code.\n(3) What is the unit of observation? In other words, what does each case mean in this data?\nAnswer: Each row in the data is a case/observation. It gives information about a country’s democracy and autocracy measures in a particular year.\n(4) According to the lecture, is this a “tidy” data?\nAnswer: Yes, the data is tidy data since it is well structured and clear. Each row is a case giving information about a country’s democracy and autocracy measures and each column is a variable representing some information about the country, the year of record, measures etc.\n\nData Transformation: use necessary commands and codes and answer the following questions.\n\n#Type your code here; and write a paragraph answering the questions.\n# unique countries\nlength(unique(polityv_data$country))\n\n[1] 195\n\n# unique years\nlength(unique(polityv_data$year))\n\n[1] 245\n\n# range and average of democ and autoc\npolityv_data %&gt;% \n  filter(democ &gt;= 0) %&gt;%\n  summarise(democ_min=min(democ), democ_max=max(democ), democ_mean=mean(democ))\n\n\n\n  \n\n\npolityv_data %&gt;% \n  filter(autoc &gt;= 0) %&gt;%\n  summarise(autoc_min=min(autoc), autoc_max=max(autoc), autoc_mean=mean(autoc))\n\n\n\n  \n\n\n# democ and autoc NA values count:\ncolSums(is.na(select(polityv_data, democ)))\n\ndemoc \n    0 \n\ncolSums(is.na(select(polityv_data, autoc)))\n\nautoc \n    0 \n\n# if we consider -88,-77,-66 as missing values too then the missing values count will be as follows:\n#length(which(polityv_data$democ == -88 | polityv_data$democ == -77 |polityv_data$democ == -66))\n\n#length(which(polityv_data$autoc == -88 | polityv_data$autoc == -77 |polityv_data$autoc == -66))\n\n(1) How many unique countries are in the data?\nAnswer: 195\n(2) How many years does this data record?\nAnswer: 245\n(3) What are the range and average of the following variables: “democ” and “autoc”?\nAnswer:\nWe filter values &gt;= 0 and use them to find the required data. ( we are using &gt;=0 condition because it was clear from the data that all the values are &gt;=0 except for -88,-77,-66 so we can use this condition without the fear of losing any other important data.\ndemoc range: [0,10]\ndemoc average: 3.501163\nautoc range: [0,10]\nautoc average: 4.02195\n** Noted that in this data, negative integers (-88, -77, and -66) represent special cases. You should exclude them when calculating the range, average, and NAs.\n(4) How many missing data (NA) are in the following variables: “democ” and “autoc”? (tips: use is.na())\nNumber of NA values in democ: 0\nNumber of NA values in autoc : 0"
  },
  {
    "objectID": "challenge_1_Fall23.html#railraod-employee-data",
    "href": "challenge_1_Fall23.html#railraod-employee-data",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Railraod Employee Data",
    "text": "Railraod Employee Data\n\nRead the dataset “railroads.xls”.\nMany government organizations still use Excel spreadsheet to store data. This railraod dataset, publisehd by the Railroad Retirement Board, is a typical example. It records the number of employees in each county and state in 2012.\nPlease load the data in R in a clean manner. You can start with doing the following things step by step.\n(1) Read the first sheet of the excel file;\n(2) Skipping the title rows;\n(3) Removing empty columns\n(4) Filtering “total” rows\n(5) Remove the table notes (the last two rows)\n\n#Type your code here\n\n\n\n\nData Description: Please use the necessary commands and codes and briefly describe this data with a short writing paragraph answering the following questions.\n\n#Type your code here; and write a paragraph answering the questions.\n\n(1) What is the dimension of the data (# of rows and columns)?\n(2) What do the rows and columns mean?\n(3) What is the unit of observation? In other words, what does each case mean in this data?\n(4) According to the lecture, is this a “tidy” data?\nData Transformation: use necessary commands and codes and answer the following questions.\n\n#Type your code here; and write a paragraph answering the questions.\n\n(1) How many unique counties and states are in the data? (tips: you can try using the across() function to do an operation on two columns at the same time)\n(2) What is the total number of employees (total_employees) in this data?\n(3) What is the min, max, mean, and median of “total_employees”\n(4) Which states have the most employees? And which countries have the most employees? (tips: use group_by() and arrange())"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "challenge_1_Fall23.html#create-your-r-quarto-project-and-submit-the-standalone-.html-file.",
    "href": "challenge_1_Fall23.html#create-your-r-quarto-project-and-submit-the-standalone-.html-file.",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Create your R quarto project and submit the standalone .html file.",
    "text": "Create your R quarto project and submit the standalone .html file.\nThis will be demonstrated in Sep 20 and 21 lab meetings."
  },
  {
    "objectID": "challenge_1_Fall23.html#part-1required.-the-baby-names-dataset",
    "href": "challenge_1_Fall23.html#part-1required.-the-baby-names-dataset",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Part 1(Required). The Baby Names Dataset",
    "text": "Part 1(Required). The Baby Names Dataset\n\nRead the dataset “babynames.csv”:\n\n\n#Type your code here\nbabynames = read.csv(\"./challenge1_data/babynames.csv\")\n\n\nData Description: Please use the necessary commands and codes and briefly describe this data with a short writing paragraph answering the following questions.\n\n#Type your code here; and write a paragraph answering the questions.\n# dimension of data:\ndim(babynames)\n\n[1] 2084710       4\n\n# To see the first few rows of the data to understand what the data is about\nhead(babynames)\n\n\n\n  \n\n\n\n(1) What is the dimension of the data (# of rows and columns)?\nAnswer: The data has 2084710 rows and 4 columns.\n(2) What do the rows and columns mean in this data?\nAnswer: The data has 4 columns - name of the baby, sex of the baby, number of occurrences of the name and the year of observation.\nBasically the data gives information about - in a particular year, how many babies have the same name for a particular gender, that is they show this count for male and female babies separately.\n(3) What is the unit of observation? In other words, what does each case mean in this data?\nAnswer: Unit of Observation is a row/observation representing a unique case of study in the data table. In this table each row is a case and it tells us information about number of same gender babies that have the same name in a given year. For example first row tells us that - in year 1880, there are 7065 female babies with the name “Mary”.\n(4) According to the lecture, is this a “tidy” data?\nAnswer: Yes. From the lecture we know that - Tidy data is a concept to describe data with a consistent form and clean structure with how it is stored. A tidy data is generally ready to be analyzed and visualized for many basic models. Coming to given data, we can observe that it has consistent and clean structure. Each row representing an observation/case and each column in the data table represent a variable giving particular information about the cases. So we can say that the given data “babynames” is tidy data.\nData Transformation: use necessary commands and codes and answer the following questions.\n\n#Type your code here; and write a paragraph answering the questions.\n\n# unique names - male, female, all:\n# we can use unique or distinct to calculate the unique names \n# using unique\nlength(unique(babynames[babynames$Sex == \"Male\", ]$Name))\n\n[1] 43653\n\nlength(unique(babynames[babynames$Sex == \"Female\", ]$Name))\n\n[1] 70225\n\nlength(unique(babynames$Name))\n\n[1] 102447\n\n#using distinct\n#nrow(distinct(filter(babynames, Sex==\"Male\"), Name))\n#nrow(distinct(filter(babynames, Sex==\"Female\"), Name))\n#nrow(distinct(babynames, Name))\n\n\n# number of years of data:\nlength(unique(babynames$Year))\n\n[1] 143\n\n#nrow(distinct(babynames, Year))\n\n\n# summarizing Occurrence column\nbabynames %&gt;% summarize(min_occ=min(Occurrences), mean_occ=mean(Occurrences), median_occ=median(Occurrences), max_occ=max(Occurrences))\n\n\n\n  \n\n\n# summarizing Occurrence column by decade\nbabynames %&gt;% \n  summarise(Occ=Occurrences, Decade=Year-(Year%%10)) %&gt;%\n  group_by(Decade) %&gt;%\n  summarise(Occurances_decade=sum(Occ)) %&gt;%\n  summarise(min_occ_decade=min(Occurances_decade), mean_occ_decade=mean(Occurances_decade), median_occ_decade=median(Occurances_decade), max_occ_decade=max(Occurances_decade))\n\n\n\n  \n\n\n\n(1) How many unique male names, unique female names, and total unique names are in the data?\nAnswer:\nMale unique names = 43653\nFemale unique names = 70225\nTotal unique names = 102447\n(2) How many years of names does this data record?\nAnswer: 143 years of data recorded.\n(3) Summarize the min, mean, median, and max of “Occurrence”. (Must use summarize())\nAnswer: min = 5, mean = 175.2112, median = 12, max = 99693\n(4) (Optional) Summarize the min, mean, median, and max of “Occurrence” by decade.\nAnswer: min = 2408032, mean = 24350971, median = 29368645, max = 39440656"
  },
  {
    "objectID": "challenge_1_Fall23.html#part-2.-choose-one-option-of-tasks-to-complete",
    "href": "challenge_1_Fall23.html#part-2.-choose-one-option-of-tasks-to-complete",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Part 2. Choose One Option of Tasks to Complete",
    "text": "Part 2. Choose One Option of Tasks to Complete\nIn this part, please choose either of the two datasets to complete the tasks."
  },
  {
    "objectID": "challenge_1_Fall23.html#optional-1-the-european-social-survey-dataset",
    "href": "challenge_1_Fall23.html#optional-1-the-european-social-survey-dataset",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Optional 1: The European Social Survey Dataset",
    "text": "Optional 1: The European Social Survey Dataset\nThe European Social Survey (ESS) is an academically-driven multi-country survey, which has been administered in over 30 countries to date. Its three aims are, firstly - to monitor and interpret changing public attitudes and values within Europe and to investigate how they interact with Europe’s changing institutions, secondly - to advance and consolidate improved methods of cross-national survey measurement in Europe and beyond, and thirdly - to develop a series of European social indicators, including attitudinal indicators.\nIn the fifth round, the survey covers 28 countries and investigates two major topics: Family Work and Wellbeing and Justice.\n\nRead the dataset “ESS_5.dta”.\n\n#Type your code here\ness5_data_full = read_dta(\"./challenge1_data/ESS_5.dta\")\n\n\n\n\nData Description: Please use the necessary commands and codes and briefly describe this data with a short writing paragraph answering the following questions.\n(1) What is the dimension of the data (# of rows and columns)?\nAnswer: The data contains - 52458 rows and 696 columns\n\n#Type your code here; and write a paragraph answering the questions.\ndim(ess5_data_full)\n\n[1] 52458   696\n\nhead(ess5_data_full)\n\n\n\n  \n\n\n\nAs we can see, this data is very large. We don’t want to study the whole data. Let’s just reload the following selected columns: “idno, essroud, male, age, edu, income_10, eth_major, media (a standardized measure of the frequency of media consumption), and cntry”.\n\n#Type your code here; and write a paragraph answering the questions.\ness5_data &lt;- select(ess5_data_full, idno, essround, male, age, edu, income_10, eth_major, media, cntry)\n\n#dimension of reloaded data\ndim(ess5_data)\n\n[1] 52458     9\n\nhead(ess5_data)\n\n\n\n  \n\n\n\n\n\n\nFor the reloaded data, what do the rows and columns mean in this data?\nAnswer: The new data has 52458 rows and 9 columns.\nEach row corresponds to an individual’s survey report. And each column contains the information about the candidate collected in the survey. idno - ID given to that candidate, essround - round of survey - given 5th, male - code for gender of candidate - 0-female : 1-male , age - age of the candidate, edu - code for education level of candidate, income_10 - code for household’s net income, eth_major - whether the candidate belongs to minority ethnic group in the country, media - a standardized measure of the frequency of a candidate’s media consumption, cntry - country the candidate belongs to.\n\n(3) What is the unit of observation? In other words, what does each case mean in this data?\nAnswer: Each row is a case/observation. It gives information about a particular candidate’s survey report like id, gender, education level, income, media utilization, country and other information relavant to the survey.\n(4) According to the lecture, is this a “tidy” data?\nAnswer: Yes. The data is well structured and clear. Each row represents an individual’s report and each column represents a variable, giving information about the candidate like id, age, country etc. Even though the data has NA values we can still call it tidy data.\nData Transformation: use necessary commands and codes and answer the following questions.\n\n#Type your code here; and write a paragraph answering the questions.\n# Here I am considering only the partial data that we created above and not the complete data \n\n# unique countries\nlength(unique(ess5_data$cntry))\n\n[1] 27\n\n#summarise data\ness5_data %&gt;% summarise(min_age=min(age, na.rm = T), max_age=max(age, na.rm = T), average_age=mean(age, na.rm = T), min_edu=min(edu, na.rm = T), max_edu=max(edu, na.rm = T), average_edu=mean(edu, na.rm = T), min_media=min(media, na.rm = T), max_media=max(media, na.rm = T), average_media=mean(media, na.rm = T))\n\n\n\n  \n\n\n# eth_major and income_10 NA values count:\ncolSums(is.na(select(ess5_data, eth_major)))\n\neth_major \n     1310 \n\ncolSums(is.na(select(ess5_data, income_10)))\n\nincome_10 \n    12620 \n\n\n(1) How many unique countries are in the data?\nAnswer: There are 27 unique countries in the reloaded data.\n(2) What are the range and average of the following variables: “age”, “edu”, and “media”? Must use summarize().\nAnswer: we are using na.rm = T to remove the NA values from our calculations\nage range = [14, 101], average = 47.91529\nedu range = [1, 4], average = 2.767531\nmedia range = [0, 1], average = 0.4786802\n(3) How many missing data (NA) are in the following variables: “eth_major” and “income_10”? (tips: use is.na())\nAnswer:\nIn eth_major column there are 1310 entries with NA values\nIn income_10 column there are 12620 entries with NA values"
  },
  {
    "objectID": "challenge_1_Fall23.html#part-3.-the-railroad-employee-data",
    "href": "challenge_1_Fall23.html#part-3.-the-railroad-employee-data",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Part 3. The Railroad Employee Data",
    "text": "Part 3. The Railroad Employee Data\n\nRead the dataset “railroads.xls”.\nMany government organizations still use Excel spreadsheets to store data. This railroad dataset, published by the Railroad Retirement Board, is a typical example. It records the number of employees in each county and state in 2012.\nPlease load the data in R in a clean manner. You can start by doing the following things step by step.\n(1) Read the first sheet of the Excel file;\n(2) Skipping the title rows;\n(3) Removing empty columns\n(4) Filtering “total” rows\n(5) Remove the table notes (the last two rows)\n\n#Type your code here\nlibrary(readxl)\nlibrary(dplyr)\n\n#1,2 loading the data and skipping the title rows\nrailroads_data &lt;- read_excel(\"./challenge1_data/railroads.xls\", sheet = 1, skip = 2)\n\n#3 removing the empty columns\nrailroads &lt;- railroads_data[-c(2,4)]\n\n# removing the empty rows\nrailroads &lt;- railroads[!apply(is.na(railroads) | railroads == \"\", 1, all),]\n\n#4 removing rows which contain \"Total\" in them\nrailroads &lt;- railroads %&gt;% filter(!grepl('Total', STATE))\n\n#5 removing the last 2 rows - table notes, can do my mentioning row numbers specifically or by using filter method\nrailroads &lt;- railroads %&gt;% filter(row_number() &lt;= n()-2)\n#railroads &lt;- railroads[-(2987:2990),] \n\n\n\n\nData Description: Please use the necessary commands and codes and briefly describe this data with a short writing paragraph answering the following questions.\n\n#Type your code here; and write a paragraph answering the questions.\n\n# dimension of data\ndim(railroads)\n\n[1] 2931    3\n\n# viewing initial few lines of data to understand what the data is about\nhead(railroads)\n\n\n\n  \n\n\n\n(1) What is the dimension of the data (# of rows and columns)?\nAnswer: Considering we removed all the empty rows, empty columns, rows with ‘Total’ in them, rows corresponding to table notes.\nThe cleaned data has 2931 rows and 3 columns.\n(2) What do the rows and columns mean?\nAnswer: There are 3 columns - State - gives alpha code of state, County - name of a county from the state and Total ( representing the number of employees). Each row gives us information about number of rail road employees in a particular county of a state. Except for the last row, which has information about whole of Canada.\n(3) What is the unit of observation? In other words, what does each case mean in this data?\nAnswer: In this data, each row is a case that is an unit of observation which gives us information about a particular county, the state it belongs to and the total number of rail road employees in that particular county.\n(4) According to the lecture, is this a “tidy” data?\nAnswer: Yes. The initial data is not clean, but once we made the necessary changes to the loaded data, the new data formed is a clean and structured data. Each row is an observation and each column is a variable giving some information about each observation. So yes, the modified data is a tidy data.\n\nData Transformation: use necessary commands and codes and answer the following questions.\n\n#Type your code here; and write a paragraph answering the questions.\n\n#1 unique states and counties\nlength(unique(railroads$STATE))\n\n[1] 54\n\nlength(unique(railroads$COUNTY))\n\n[1] 1710\n\n#2 total number of employees\ncolSums(railroads[,c(3)])\n\n TOTAL \n256094 \n\n#3 min, max, mean and median of total_employees\nrailroads %&gt;% summarize(min=min(TOTAL), mean=mean(TOTAL), median=median(TOTAL), max=max(TOTAL))\n\n\n\n  \n\n\n#4 counties and states with most employees: here list given in desc order \narrange(railroads, desc(TOTAL))\n\n\n\n  \n\n\nrailroads %&gt;% \n  group_by(STATE) %&gt;%\n  summarise(sumTotal=sum(TOTAL)) %&gt;%\n  arrange(desc(sumTotal), .by_group=TRUE)\n\n\n\n  \n\n\n\n(1) How many unique counties and states are in the data? (tips: you can try using the across() function to do an operation on two columns at the same time)\nAnswer:\nNumber of unique states = 54\nNumber of unique counties = 1710\n(2) What is the total number of employees (total_employees) in this data?\nAnswer: total number of employees in this data = 256094\n(3) What are the min, max, mean, and median of “total_employees”\nAnswer:\nmin = 1\nmean = 87.37\nmedian = 21\nmax = 8207\n(4) Which states have the most employees? And which countries have the most employees? (tips: use group_by() and arrange())\nAnswer:\nTop 5 Counties with the most employees = “COOK”, “TARRANT”, “DOUGLAS”, “SUFFOLK”, “INDEPENDENT CITY”.\nTop 5 States with the most employees = “TX”, “IL”, “NY”, “NE”, “CA”."
  },
  {
    "objectID": "challenge_2_Fall23.html",
    "href": "challenge_2_Fall23.html",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "",
    "text": "Make sure you change the author’s name in the above YAML header."
  },
  {
    "objectID": "challenge_2_Fall23.html#setup",
    "href": "challenge_2_Fall23.html#setup",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "Setup",
    "text": "Setup\nIf you have not installed the following packages, please install them before loading them.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(haven) #for loading other datafiles (SAS, STATA, SPSS, etc.)\nlibrary(stringr) # if you have not installed this package, please install it.\nlibrary(lubridate)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "challenge_2_Fall23.html#challenge-overview",
    "href": "challenge_2_Fall23.html#challenge-overview",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nBuilding on the lectures in week#3 and week#4, we will continually practice the skills of different transformation functions with Challenge_2. In addition, we will explore the data more by conducting practices with pivoting data and dealing with date-time data.\nThere will be coding components and writing components. Please read the instructions for each part and complete your challenges."
  },
  {
    "objectID": "challenge_2_Fall23.html#datasets",
    "href": "challenge_2_Fall23.html#datasets",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "Datasets",
    "text": "Datasets\nThere are four datasets provided in this challenge. Please download the following dataset files from Canvas or Google Classroom and save them to a folder within your project working directory (i.e.: “yourworkingdiectory_data”). If you don’t have a folder to store the datasets, please create one.\n\nESS_5.dta (Part 1) ⭐\np5v2018.sav (Part 1)⭐\naustrlian_data.csv (Part 3)⭐\nFedFundsRate.csv (Part 4)⭐\n\nFind the _data folder, then use the correct R command to read the datasets."
  },
  {
    "objectID": "challenge_2_Fall23.html#part-1required.-depending-on-the-data-you-chose-in-challenge1-ess_5-or-polity-v-please-use-that-data-to-complete-the-following-tasks",
    "href": "challenge_2_Fall23.html#part-1required.-depending-on-the-data-you-chose-in-challenge1-ess_5-or-polity-v-please-use-that-data-to-complete-the-following-tasks",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "Part 1(Required). Depending on the data you chose in Challenge#1 (ESS_5 or Polity V), please use that data to complete the following tasks",
    "text": "Part 1(Required). Depending on the data you chose in Challenge#1 (ESS_5 or Polity V), please use that data to complete the following tasks"
  },
  {
    "objectID": "challenge_2_Fall23.html#if-you-are-using-the-ess_5-data",
    "href": "challenge_2_Fall23.html#if-you-are-using-the-ess_5-data",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "If you are using the ESS_5 Data:",
    "text": "If you are using the ESS_5 Data:\n\nRead the dataset and keep the first 39 columns.\n\n\n#Type your code here\ness_data_org &lt;- read_dta(\"./challenge1_data/ESS_5.dta\") %&gt;%  \n  select(1:39)\n\ndim(ess_data_org)\n\n[1] 52458    39\n\n\n\nConduct the following transformation for the data by using mutate() and other related functions :\n(1) Create a new column named “YearOfBirth” using the information in the “age” column.\n\n#Type your code here\n#1 Creating \"YearOfBirth\" from age column\ncurr_year &lt;- year(Sys.Date())\ness_data_mod &lt;- ess_data_org %&gt;%\n  mutate(YearOfBirth = curr_year - age)\n\n\n\n\nCreate a new column named “adult” using the information in the “age” column.\n\n\n#2 Creating \"adult\" from age column\ness_data_mod &lt;- ess_data_mod %&gt;%\n  mutate(adult = ifelse(age &gt;= 18, \"Yes\", \"No\"))\n\n\n\n\nRecode the “commonlaw” column: if the value is 0, recode it as “non-common-law”; if the value is 1, recode it as “common-law”.\n\n\n#3 Recoding \"commonlaw\" column\ness_data_mod &lt;- ess_data_mod %&gt;%\n  mutate(commonlaw = ifelse(commonlaw == 0, \"non-common-law\", \"common-law\"))\n\n\n\n\nRecode the “vote” column: if the value is 3, recode it as 1; if the value is smaller than 3, recode it as 0. Make sure to exclude the NAs.\n\n\n#4 Recoding \"vote\" column\ness_data_mod &lt;- ess_data_mod %&gt;%\n  mutate(vote = case_when(\n    !is.na(vote) & vote == 3 ~ 1,\n    !is.na(vote) & vote &lt; 3 ~ 0,\n    TRUE ~ NA\n  ))\n\n\n\n\nMove the column “YearOfBirth”, “adult,” “commonlaw” and “vote” right after the “essround” column (the 2nd column in order).\n\n\n#5 Moving columns\ness_data_mod &lt;- ess_data_mod %&gt;%\n  select(1:2, YearOfBirth, adult, commonlaw, vote, everything())\n\n\n\n\nAnswer the question: What is the data type of the “commonlaw” column before and after recoding? And what is the data type of the “vote” column before and after recoding?\n\n\n#6 Data type before and after recoding\ncat(\"commonlaw data type before recoding: \", typeof(ess_data_org$commonlaw), \"\\n\")\n\ncommonlaw data type before recoding:  double \n\ncat(\"commonlaw data type after recoding: \", typeof(ess_data_mod$commonlaw), \"\\n\")\n\ncommonlaw data type after recoding:  character \n\ncat(\"vote data type before recoding: \", typeof(ess_data_org$vote), \"\\n\")\n\nvote data type before recoding:  double \n\ncat(\"vote data type after recoding: \", typeof(ess_data_mod$vote), \"\\n\")\n\nvote data type after recoding:  double"
  },
  {
    "objectID": "challenge_2_Fall23.html#if-you-are-using-the-polity-v-data---not-using-this.",
    "href": "challenge_2_Fall23.html#if-you-are-using-the-polity-v-data---not-using-this.",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "** If you are using the Polity V Data: - Not using this.**",
    "text": "** If you are using the Polity V Data: - Not using this.**\n\nRead the dataset and keep the first 11 columns.\n\n\n#Type your code here\n\n\nConduct the following transformation for the data by using mutate() and other related functions :\n(1) Create a new column named “North America” using the information in the “country” column. Note: “United States,” “Mexico,” or “Canada” are the countries in North America. In the new “North America” column, if a country is one of the above three countries, it should be coded as 1, otherwise as 0.\n(2) Recode the “democ” column: if the value is 10, recode it as “Well-Functioning Democracy”; if the value is greater than 0 and smaller than 10, recode it as “Either-Autocracy-or-Democracy”; if the value is 0, recode it as “Non-democracy”; if the value is one of the following negative integers (-88, -77, and -66), recode it as “Special-Cases.”\n(3) Move the column “North America” and “democ” right before the “year” column (the 6th column in order).\n(4) Answer the question: What is the data type of the “North America” column? What is the data type of the “democ” column before and after recoding?\n\n\n#Type your code here"
  },
  {
    "objectID": "challenge_2_Fall23.html#part-2.-generate-your-own-data",
    "href": "challenge_2_Fall23.html#part-2.-generate-your-own-data",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "Part 2. Generate your own Data",
    "text": "Part 2. Generate your own Data\n\nGenerate an untidy data that includes 10 rows and 10 columns. In this dataset, column names are not names of variables but a value of a variable.\n*Note: do not ask ChatGPT to generate a dataframe for you. I have already checked the possible questions and answers generated by AI.\n\n\nactors_data_untidy &lt;- data.frame(\n  Year = 2011:2020,\n  `Adam_Sandler` = c(4, 3, 1, 4, 4, 1, 3, 4, 2, 4),\n  `Rock` = c(1, 1, 5, 1, 3, 2, 4, 2, 4, 0),\n  `Robert_Downey_Jr` = c(1, 1, 1, 2, 1, 1, 1, 1, 1, 1),\n  `Chris_Evans` = c(3, 2, 2, 2, 3, 1, 2, 1, 4, 0),\n  `Scarlett_Johansson` = c(2, 2, 3, 3, 1, 4, 2, 2, 4, 0),\n  `Kevin_Hart` = c(3, 3, 3, 5, 2, 4, 3, 1, 3, 0),\n  `Chris_Hemsworth` = c(1, 4, 3, 0, 4, 4, 2, 3, 3, 1),\n  `Jennifer_Lawrence` = c(3, 3, 3, 3, 2, 3, 1, 1, 1, 0),\n  `Emma_Watson` = c(2, 1, 1, 1, 2, 0, 2, 0, 1, 0)\n  #`Will_Smith` = c(0, 1, 2, 1, 2, 2, 1, 0, 5, 1)\n)\n\nprint(actors_data_untidy)\n\n   Year Adam_Sandler Rock Robert_Downey_Jr Chris_Evans Scarlett_Johansson\n1  2011            4    1                1           3                  2\n2  2012            3    1                1           2                  2\n3  2013            1    5                1           2                  3\n4  2014            4    1                2           2                  3\n5  2015            4    3                1           3                  1\n6  2016            1    2                1           1                  4\n7  2017            3    4                1           2                  2\n8  2018            4    2                1           1                  2\n9  2019            2    4                1           4                  4\n10 2020            4    0                1           0                  0\n   Kevin_Hart Chris_Hemsworth Jennifer_Lawrence Emma_Watson\n1           3               1                 3           2\n2           3               4                 3           1\n3           3               3                 3           1\n4           5               0                 3           1\n5           2               4                 2           2\n6           4               4                 3           0\n7           3               2                 1           2\n8           1               3                 1           0\n9           3               3                 1           1\n10          0               1                 0           0\n\ndim(actors_data_untidy)\n\n[1] 10 10\n\n\n\nUse the correct pivot command to convert the data to tidy data.\n\n\n#Type your code here\nactors_data_tidy &lt;- actors_data_untidy %&gt;%\n  pivot_longer(cols = -Year, names_to = \"Actor\", values_to = \"movies_count\")\n\nprint(actors_data_tidy)\n\n# A tibble: 90 × 3\n    Year Actor              movies_count\n   &lt;int&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1  2011 Adam_Sandler                  4\n 2  2011 Rock                          1\n 3  2011 Robert_Downey_Jr              1\n 4  2011 Chris_Evans                   3\n 5  2011 Scarlett_Johansson            2\n 6  2011 Kevin_Hart                    3\n 7  2011 Chris_Hemsworth               1\n 8  2011 Jennifer_Lawrence             3\n 9  2011 Emma_Watson                   2\n10  2012 Adam_Sandler                  3\n# ℹ 80 more rows\n\ndim(actors_data_tidy)\n\n[1] 90  3\n\n\n\nGenerate an untidy data that includes 10 rows and 5 columns. In this dataset, an observation is scattered across multiple rows.\n\n\n#Type your code here\nstudent_data_untidy &lt;- data.frame(\n  student_id = c(2016001,2016001,2016002,2016002,2016003,2016003,2016004,2016004,2016005,2016005),\n    year = c(2016,2016,2016,2016,2016,2016,2016,2016,2016,2016),\n  student_name = c(\"Rick\", \"Rick\", \"Andy\", \"Andy\", \"James\", \"James\", \"Ryan\", \"Ryan\", \"Evan\", \"Evan\"),\n  category = c(\"grade\", \"attandance\", \"grade\", \"attandance\", \"grade\", \"attandance\", \"grade\", \"attandance\", \"grade\", \"attandance\" ),\n  value = c(\"A+\",95,\"A\",80,\"B\",91,\"B+\",86,\"A-\",96) \n)\n\nprint(student_data_untidy)\n\n   student_id year student_name   category value\n1     2016001 2016         Rick      grade    A+\n2     2016001 2016         Rick attandance    95\n3     2016002 2016         Andy      grade     A\n4     2016002 2016         Andy attandance    80\n5     2016003 2016        James      grade     B\n6     2016003 2016        James attandance    91\n7     2016004 2016         Ryan      grade    B+\n8     2016004 2016         Ryan attandance    86\n9     2016005 2016         Evan      grade    A-\n10    2016005 2016         Evan attandance    96\n\ndim(student_data_untidy)\n\n[1] 10  5\n\n\n\nUse the correct pivot command to convert the data to tidy data.\n\n\n#Type your code here\nstudent_data_tidy &lt;- student_data_untidy %&gt;%\n  pivot_wider(names_from = category, values_from = value)\nstudent_data_tidy$attandance &lt;- as.numeric(student_data_tidy$attandance )\n\nprint(student_data_tidy)\n\n# A tibble: 5 × 5\n  student_id  year student_name grade attandance\n       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt;\n1    2016001  2016 Rick         A+            95\n2    2016002  2016 Andy         A             80\n3    2016003  2016 James        B             91\n4    2016004  2016 Ryan         B+            86\n5    2016005  2016 Evan         A-            96\n\ndim(student_data_tidy)\n\n[1] 5 5"
  },
  {
    "objectID": "challenge_2_Fall23.html#part-3.-the-australian-data",
    "href": "challenge_2_Fall23.html#part-3.-the-australian-data",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "Part 3. The Australian Data",
    "text": "Part 3. The Australian Data\nThis is another tabular data source published by the Australian Bureau of Statistics that requires a decent amount of cleaning. In 2017, Australia conducted a postal survey to gauge citizens’ opinions towards same sex marriage: “Should the law be changed to allow same-sex couples to marry?” All Australian citizens are required to vote in elections, so citizens could respond in one of four ways: vote yes, vote no, vote in an unclear way(illegible), or fail to vote. (See the “Explanatory Notes” sheet for more details.)\nI have already cleaned up the data for you and you can directly import it. We will come back to clean and process the original “messy” data after we learn some string functions in the later weeks.\n\nRead the dataset “australian_data.csv”:\n\n\n#Type your code here\naustralian_data_untidy &lt;- read.csv(\"./challenge2_data/australian_data.csv\", header=TRUE, row.names = 1, check.names = FALSE)\n\n\nData Description: Please use the necessary commands and codes and briefly describe this data with a short writing paragraph answering the following questions.\n\n#Type your code here\n#1 dimension of data\ndim(australian_data_untidy)\n\n[1] 150   6\n\n#2\nhead(australian_data_untidy)\n\n\n\n  \n\n\n\n(1) What is the dimension of the data (# of rows and columns)?\nAnswer: The data contains 150 rows and 6 columns. Dimension is 150x6.\n(2) What do the rows and columns mean in this data?\nAnswer: The column gives information about the following: District: name of the district, Yes: number of citizens voted Yes, No: number of citizens voted No, Illegible: number of citizen’s votes that are unclear, No Response: number of citizens that did not participate in voting, Division: the division that the district belongs to.\nEach row gives citizen’s voting information about a particular district in Australia that is number of people who voted yes and no, people whose vote was unclear to read, number of people who did not vote at all along with the division the district belongs to.\n(3) What is the unit of observation? In other words, what does each case mean in this data?\nAnswer: Here each row is a case/observation which tells us about a particular district’s voting information on the law allowing same sex couple marriage. It gives us data about number of “Yes” votes, “No” votes, illegible votes and number of people who did not participate in voting. It also tells us the division that the district belongs to.\n(4) According to the lecture, is this a “tidy” data? Why?\nAnswer: This is not a tidy data. In tidy data all the columns represent different variables but here the columns “Yes”,“No”,“Illegible”,“No Response” are not variables but different values for the variable “Vote_category”. We can modify this to form a tidy data as shown below.\n(5) If this is not a tidy data, please use the necessary commands to make it “tidy”.\nAnswer: To make this a tidy data, we have to convert the columns “Yes”,“No”,“Illegible” and “No Response” to “vote_category” and “vote_count” columns.\n\naustralian_data &lt;- australian_data_untidy %&gt;%\n  gather(key=\"vote_category\",value=\"vote_count\", Yes, No, Illegible, `No Response`)\n\ndim(australian_data)\n\n[1] 600   4\n\nhead(australian_data)\n\n\n\n  \n\n\n\nData Transformation: use necessary commands and codes and answer the following questions. If you reshape the data in the previous step, please work on the reshaped data.\n\n#Type your code here\n#1\n#Number of Districts\nnrow(unique(australian_data[, c(\"District\", \"Division\")]))\n\n[1] 150\n\n#Number of divisions\nlength(unique(australian_data$Division))\n\n[1] 8\n\n#2 Creating new column \"district turnout(%)\"\naustralian_data &lt;- australian_data %&gt;%\n  group_by(District) %&gt;%\n  mutate(total_votes = sum(vote_count[vote_category %in% c(\"Yes\", \"No\", \"Illegible\")])) %&gt;%\n  mutate(total_population = sum(vote_count)) %&gt;%\n  mutate(district_turnout = (total_votes / total_population) * 100) %&gt;%\n  ungroup()\n\n#3 \n\n#Number of supporters and opposers\naustralian_data %&gt;% \n  filter(vote_category == \"Yes\") %&gt;%\n  summarise(`Number of Supporters` = sum(vote_count))\n\n\n\n  \n\n\naustralian_data %&gt;% \n  filter(vote_category == \"No\") %&gt;%\n  summarise(`Number of Opposers` = sum(vote_count))\n\n\n\n  \n\n\n# District with maximum supporters\naustralian_data %&gt;% \n  filter(vote_category == \"Yes\") %&gt;%\n  summarise(`District with maximum supporters` = District[which.max(vote_count)], \"votes_count\"=max(vote_count))\n\n\n\n  \n\n\n# Division with highest yes %\naustralian_data %&gt;% \n  group_by(Division) %&gt;%\n  summarise(Division_approval_rate= sum(vote_count[vote_category==\"Yes\"])*100/(sum(vote_count[vote_category %in% c(\"Yes\",\"No\",\"Illegible\")]))) %&gt;%\n  summarise(`Division with highest Yes %` = Division[which.max(Division_approval_rate)])\n\n\n\n  \n\n\n# Average approval rate at division level\n# Assumption 1\naustralian_data %&gt;% \n  group_by(Division) %&gt;%\n  summarise(Division_approval_rate= sum(vote_count[vote_category==\"Yes\"])*100/(sum(vote_count[vote_category %in% c(\"Yes\",\"No\",\"Illegible\")]))) %&gt;%\n  summarise(`average approval rate at Divison level` = mean(Division_approval_rate))\n\n\n\n  \n\n\n# Assumption 2\naustralian_data %&gt;% \n  group_by(District) %&gt;%\n  summarise(division=Division, approval_rate =sum(vote_count[vote_category==\"Yes\"])*100/(sum(vote_count[vote_category %in% c(\"Yes\",\"No\",\"Illegible\")]))  ) %&gt;%\n  group_by(division) %&gt;%\n  summarise(Avg_approval_rate = mean(approval_rate)) %&gt;%\n  arrange(desc(Avg_approval_rate), .by_group=TRUE)\n\n\n\n  \n\n\n\n(1) How many districts and divisions are in the data?\nAnswer:\nSince multiple districts can have same, we use the combination of district and division to find unique districts. This will give us correct number even if districts from different divisions have same name.\nNumber of districts: 150\nNumber of divisions: 8\n(2) Use mutate() to create a new column “district turnout(%)”. This column should be the voting turnout in a given district, or the proportion of people cast votes (yes, no, and illegible) in the total population of a district.\nAnswer: The command is shown above.\n(3) please use summarise() to estimate the following questions:\n\nIn total, how many people support same-sex marriage in Australia, and how many people oppose it?\nAnswer:\nNumber of Supporters = 7817247\nNumber of Opposers = 4873987\n\nWhich district has most people supporting the policy, and how many?\nAnswer:\nDistrict with most people supporting the policy: Canberra(d)\nNumber of supporters from that district: 89590\n\nWhich division has the highest approval rate (% of “yes” in the total casted votes)? And what is the average approval rate at the division level?\nAnswer:\nDivision with highest Yes % = Australian Capital Territory Divisions\n\nSince average approval at division level definition is not clearly stated, I found the following 2 set of values.\n1.Found approval rate for each division and then took the average for all the divisions, this value is 63.30475\n2.Finding approval rate for each district and then finding average of all the districts’ approval rates that belong to a given division. Those values are given below:\nAverage approval rate at division level:\nAustralian Capital Territory Divisions = 73.87287\nVictoria Divisions = 64.44427\nWestern Australia Divisions = 63.44548\nTasmania Divisions = 63.22453\nSouth Australia Divisions = 62.05639\nQueensland Divisions = 60.23834\nNorthern Territory Divisions = 59.69649\nNew South Wales Divisions = 57.07813"
  },
  {
    "objectID": "challenge_2_Fall23.html#part-4.-the-marco-economic-data",
    "href": "challenge_2_Fall23.html#part-4.-the-marco-economic-data",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "Part 4. The Marco-economic Data",
    "text": "Part 4. The Marco-economic Data\nThis data set runs from July 1954 to March 2017, and includes daily macroeconomic indicators related to the effective federal funds rate - or the interest rate at which banks lend money to each other in order to meet mandated reserve requirements.\n\nRead the dataset “FedFundsRate.csv”:\n\n\n#Type your code here\nfed_rates_org &lt;- read.csv(\"./challenge2_data/FedFundsRate.csv\", header=TRUE, check.names = FALSE)\n\n\nData Description: Please use the necessary commands and codes and briefly describe this data with a short writing paragraph answering the following questions.\n\n#Type your code here\ndim(fed_rates_org)\n\n[1] 904  10\n\nhead(fed_rates_org)\n\n\n\n  \n\n\n\n(1) What is the dimension of the data (# of rows and columns)?\nAnswer: Data has 904 rows and 10 columns. Dimension is 904x10.\n(2) What do the rows and columns mean in this data?\nAnswer: The columns represent the following: Year,Month,Day - gives us the date for which the federal rates are being shown.\n“Federal Funds Target Rate” - represents the target rate for that date,\n“Federal Funds Upper Target” - represents the upper bound for that date,\n“Federal Funds Lower Target” - represents the lower bound for that date,\n“Effective Federal Funds Rate” - represents effective rate for that date,\n“Real GDP (Percent Change)” - represents percentage change in GDP on that date,\n“Unemployment Rate” - tells us information about unemployment on that date,\n“Inflation Rate” - gives the inflation rate on that date.\nEach row gives us information about macro economic indicators related to effective federal rates for a given date.\n(3) What is the unit of observation? In other words, what does each case mean in this data?\nAnswer: Here each row is a case/observation. Unit of observation is a date and corresponding macro economic indicators related to federal rates.\nGenerating a date column:\nNotice that the year, month, and day are three different columns. We will first have to use a string function called “str_c()” from the “stringr” library to combine these three columns into one “date” column. Please revise the following commands\n\nfed_rates &lt;- fed_rates_org %&gt;%\n  mutate(date = str_c(Year, Month, Day, sep=\"-\"))\n\nMove the new created “date” column to the beginning as the first column of the data.\n\n#Type your code here\nfed_rates &lt;- fed_rates %&gt;%\n  select(date, everything())\n\nWhat is the data type of the new “date” column?\n\n#Type your code here\nclass(fed_rates$date)\n\n[1] \"character\"\n\n\nTransform the “date” column to a &lt;date&gt; data.\n\n#Type your code here\nfed_rates &lt;- fed_rates %&gt;%\n  mutate(date = as.Date(date, format = \"%Y-%m-%d\"))\n\nclass(fed_rates$date)\n\n[1] \"Date\"\n\n\nConduct following statistics:\n\n#Type your code here\n# Dates of highest unemployment ratee\nfed_rates %&gt;%\n  summarise(\"Dates of highest unemployment rate\" = date[!is.na(`Unemployment Rate`) & `Unemployment Rate` == max(`Unemployment Rate`, na.rm = TRUE)] )\n\n\n\n  \n\n\n# Dates of lowest unemployment rate\nfed_rates %&gt;%\n  summarise(\"Dates of lowest unemployment rate\" = date[!is.na(`Unemployment Rate`) & `Unemployment Rate` == min(`Unemployment Rate`, na.rm = TRUE)] )\n\n\n\n  \n\n\n\n(1) On which date is the highest unemployment rate? and the lowest?\nAnswer:\nThere are multiple dates with highest and lowest unemployment rates, listed them below:\nDates of highest unemployment rate = 1982-11-01, 1982-12-01\nDates of lowest unemployment rate = 1968-09-01, 1968-10-01, 1968-11-01, 1968-12-01, 1969-01-01, 1969-02-01, 1969-03-01, 1969-04-01, 1969-05-01\n(2) (Optional) Which decade has the highest average unemployment rate?\nAnswer: The decade 1974-1983 has the highest unemployment rate.\n\nHere is a template for you to create a decade column to allow you to group the data by decade. You can use it for the optional question in Challenge#1:\n\nfed_rates &lt;- fed_rates |&gt;\n  mutate(Decade = cut(Year, breaks = seq(1953, 2027, by = 10), labels = format(seq(1954, 2017, by = 10), format = \"%Y\")))\n\nfed_rates %&gt;%\n  group_by(Decade) %&gt;%\n  summarise(\"average_unemployment_rate\" = mean(`Unemployment Rate`, na.rm = TRUE)) %&gt;%\n  arrange(desc(average_unemployment_rate), .by_group=TRUE)\n\n\n\n  \n\n\n#using different definition of decade\n#fed_rates %&gt;%\n#  mutate(Decade = Year-Year%%10) %&gt;%\n#  group_by(Decade) %&gt;%\n#  summarise(avg_unemployment_rate = mean(`Unemployment Rate`, na.rm = TRUE)) %&gt;%\n#  arrange(desc(avg_unemployment_rate), .by_group=TRUE)\n\n\n\n##Note: the cut() a baseR function that we don't generally use. Basically, it allows us divides the range of Year into intervals and codes the values in Year according to which interval (1954 and 2017) they fall; the break argument specifies how we segmate the sequence of Year (by a decade)"
  },
  {
    "objectID": "challenge_3_Fall23.html",
    "href": "challenge_3_Fall23.html",
    "title": "Challenge_3: Joining Relational Data, Writing Your Own Functions, and String Operations",
    "section": "",
    "text": "Make sure you change the author’s name in the above YAML header."
  },
  {
    "objectID": "challenge_3_Fall23.html#setup",
    "href": "challenge_3_Fall23.html#setup",
    "title": "Challenge_3: Joining Relational Data, Writing Your Own Functions, and String Operations",
    "section": "Setup",
    "text": "Setup\nIf you have not installed the following packages, please install them before loading them.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(haven) #for loading other datafiles (SAS, STATA, SPSS, etc.)\nlibrary(stringr) # if you have not installed this package, please install it.\nlibrary(lubridate)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "challenge_3_Fall23.html#challenge-overview",
    "href": "challenge_3_Fall23.html#challenge-overview",
    "title": "Challenge_3: Joining Relational Data, Writing Your Own Functions, and String Operations",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nIn this challenge, we will practice join() with relational data. We will also explore some string functions to process, extract information, and mutate and clean data.\nThere will be coding components and writing components. Please read the instructions for each part and complete your challenges."
  },
  {
    "objectID": "challenge_3_Fall23.html#datasets",
    "href": "challenge_3_Fall23.html#datasets",
    "title": "Challenge_3: Joining Relational Data, Writing Your Own Functions, and String Operations",
    "section": "Datasets",
    "text": "Datasets\nThere are four datasets provided in this challenge. Please download the following dataset files from Canvas or Google Classroom and save them to a folder within your project working directory (i.e.: “yourworkingdiectory_data”). If you don’t have a folder to store the datasets, please create one.\n\nPart 1 and 2: ESS_5.dta and p5v2018.sav (used in Challenge#1) ⭐⭐\nPart 3: babynames.csv (used in Challenge#1) ⭐\nPart 4: australian_marriage_law_postal_survey_2017_-_response_final.xls ⭐⭐⭐\n\nFind the _data folder, then use the correct R command to read the datasets."
  },
  {
    "objectID": "challenge_3_Fall23.html#part-1.-joining-individual-level-and-country-level-data",
    "href": "challenge_3_Fall23.html#part-1.-joining-individual-level-and-country-level-data",
    "title": "Challenge_3: Joining Relational Data, Writing Your Own Functions, and String Operations",
    "section": "Part 1. Joining Individual-level and Country-Level Data",
    "text": "Part 1. Joining Individual-level and Country-Level Data\nWe have been working with these two datasets in the previous two challenges and should be familiar with one. Suppose we have a research project that studies European citizens’ social behaviors and public opinions, and we are interested in how the countries that respondents live in influence their behavior and opinion. In this case, we will need to combine the two data for future analysis.\n\nRead the two raw datasets.\nFor ESS_5: (1) keep only the following columns: idno, essround, male, age, edu, eth_major, income_10, cntry, vote. (2) recode essround to 2010, and rename it as year.\nFor Polity V, keep the first 10 columns.\n\n\n#Type your code here\ness_data_org &lt;- read_dta(\"./challenge3_data/ESS_5.dta\") %&gt;%\n  select(idno, essround, male, age, edu, eth_major, income_10, cntry, vote)\ness_data_org &lt;- ess_data_org %&gt;%\n  mutate(essround = 2010) %&gt;%\n  rename(year = essround)\n\nhead(ess_data_org)\n\n\n\n  \n\n\npolity_data &lt;- read_sav(\"./challenge3_data/p5v2018.sav\") %&gt;%\n  select(1:10)\n\nhead(polity_data)\n\n\n\n  \n\n\n\n\nAnswer the following questions:\n(1) In this project, which is the primary data, and which is the foreign data?\nAnswer:\nHere primary data is ESS_5 and foreign data is Polity V data.\nAs mentioned, in current project our main aim is to study European citizens’ social behaviors and public opinions, and see how the countries that respondents live in influence their behavior. Since ESS_5 data has individual’s information like gender, age, education, income etc - this is our main focus so this is the primary data. On the other hand, polity data has information about countries and information about countries - we are trying to study this data’s influence on primary data, so this will be our foreign data.\n(2) What is(are) the key(s) for the two data?\nAnswer:\nFor ESS_5 data: A combination of “idno” and “cntry” is the key. Together they will contribute to unique observation.\nFor Polity V data: “cyear” is the key or we can use a combination of country and year. Note that cyear is a combination of country and year so this alone will be sufficient to represent a unique observation.\nSuppose we have a theory that a country’s level of democracy (democ in Polity V) affects an individual’s electoral participation (vote in ESS 5). We must first conduct some necessary data transformation before merging the two data.\n(1) Countries in ESS_5 are coded with their 2-digit codes (ISO-3166-1) in the cntry column. It is difficult to identify from these two-letter abbreviations. Let’s first transform the cntry column by changing it from the abbreviations to the full country names and renaming the column as country.\nPlease refer to this website for the list of countries with their 2-letter abbreviations. There are two ways to accomplish this task, and you can choose either one:\n\nmanually recode each country abbreviation to its full name or\ndownload the country list (csv) file from the above website, import it in RStudio, and merge it with the ESS_5 data. By doing so, you automatically join a new “country” column to the existing ESS_5 data.\n\n\n#Type your code here\ncountry_data &lt;- read_csv(\"./challenge3_data/country_codes.csv\", show_col_types = FALSE) %&gt;%\n  rename(cntry = Code, country = Name)\n\n# since we are asked to rename the cntry column, we will just remove the cntry column because we have the newly created country column.\ness_data &lt;- left_join(ess_data_org, country_data, by=\"cntry\") %&gt;%\n  select(-cntry) %&gt;%\n  select(1:2, country, everything())\n\nhead(ess_data)\n\n\n\n  \n\n\n\n(2) What column(s) will we use as a matching key(s) for combining the two data? Note: you can use multiple matching strategies, but I suggest we create a common matching key for both data if there are none.\nAnswer: We can use a combination of country and year as matching keys for combining the two data.\n(3) Join the two data (ESS_5 and Polity V). Please print the first few entries as a sanity check. Name the joined data as “ESS_Polity”\n\n#Type your code here\nESS_Polity &lt;- left_join(ess_data, polity_data, by = c(\"country\", \"year\"))\n\nhead(ESS_Polity)\n\n\n\n  \n\n\n\n(4) Save the joined data ESS_Polity to your local directory using the following code. We will be using this joined data to explore visualization in future challenges.\n\nwrite_csv(ESS_Polity, \"ESS_Polity.csv\")\n\nDescribe the data structure of the newly joined data ESS_Polity. What is its dimension (# of rows and # of columns)? What is its unit of observation? Compared to the original ESS_5 data, does the above data combination change the dimension and unit of observation?\n\n#Type your code here\ndim(ESS_Polity)\n\n[1] 52458    17\n\nhead(ESS_Polity)\n\n\n\n  \n\n\ncolnames(ESS_Polity)\n\n [1] \"idno\"      \"year\"      \"country\"   \"male\"      \"age\"       \"edu\"      \n [7] \"eth_major\" \"income_10\" \"vote\"      \"p5\"        \"cyear\"     \"ccode\"    \n[13] \"scode\"     \"flag\"      \"fragment\"  \"democ\"     \"autoc\"    \n\n\nAnswer:\nThe dimension new polity data is 52458x17. It has 52458 rows and 17 columns.\nThe unit of observation here an individual’s response in the survey from a particular country in a particular year - which gives information about country’s level of democracy.\nEach row gives information about an individual taking the survey like age, education, income, gender and the individual’s country’s information like democracy and autocracy measures.\n\nThe dimension of original ESS_5 data is 52458x696. But since we only selected few columns the dimensions after selecting specific columns is 52458x9.\nDimension of ESS_Polity is 52458x17.\nUnit of observation in ESS_5 data is an individual’s survey response. Unit of observation in ESS_Polity data is an individual’s survery response from a particular country in a particular year. Yes the dimension of the data and the unit of observation, both changed after merging.\n\n(Optional) Suppose our focus is studying regimes and governments in different countries (Polity V data). Particularly, we are interested in the relationship between the average education level in each country and the level of democracy in that country. What is the primary and foreign data in this study? How will you combine the two data?\nAnswer:\nIn this study, the primary data will be Polity V since our focus is on country’s specific measures. ESS_5 data will be foreign data since we are trying to study an individual’s influence on a country.\n\nSince in ESS_5 data, for a given country we have so many individual’s information, we need to first combine them and then merge them with polity data. How we summarize the individual’s data is specific to a given problem. Here we just find the average of their education level and their age.\n\n#Type your code here\ness_grouped &lt;- ess_data %&gt;%\n  group_by(country) %&gt;%\n  summarise(avg_age = mean(age, na.rm = TRUE), avg_edu = mean(edu, na.rm = TRUE))\n\n#merge data\nPolity_ESS &lt;- left_join(polity_data, ess_grouped, by = \"country\")\n\nhead(Polity_ESS)"
  },
  {
    "objectID": "challenge_3_Fall23.html#part-2.-writing-your-own-functions",
    "href": "challenge_3_Fall23.html#part-2.-writing-your-own-functions",
    "title": "Challenge_3: Joining Relational Data, Writing Your Own Functions, and String Operations",
    "section": "Part 2. Writing Your Own Functions",
    "text": "Part 2. Writing Your Own Functions\nPlease use the joined data ESS_Polity in Part 1 and write a function to complete all the following tasks:\n(1) Estimate the range, average, and standard deviation of any given numeric-type (double or integer) columns.\n(2) Estimate the number of NAs and the number of unique values of any given column.\n(3) Test your function with any four columns of your choice.\n\n#Type your code here\nnumeric_column_analysis &lt;- function(column_name){\n  dataframe &lt;- ESS_Polity\n  if(!is.numeric(dataframe[[column_name]])){\n    cat(\"The given column is not numeric - cannot perform the requested analysis\\n\")\n    return(NULL)\n  }\n  cat(\"For Column: \", column_name, \"\\n\")\n  mean_value &lt;- mean(dataframe[[column_name]], na.rm = TRUE)\n  min_value &lt;- min(dataframe[[column_name]], na.rm = TRUE)\n  max_value &lt;- max(dataframe[[column_name]], na.rm = TRUE)\n  sd_value &lt;- sd(dataframe[[column_name]], na.rm = TRUE)\n  cat(\"Range: [\",min_value,\",\",max_value,\"] \\nAverage:\", mean_value,\" \\nStandard Deviation: \", sd_value,\"\\n\")\n}\n\ncount_unique_na &lt;- function(column_name){\n  dataframe &lt;- ESS_Polity\n  cat(\"In Column: \", column_name, \"\\n\")\n  unique_count &lt;- length(unique(dataframe[[column_name]]))\n  na_count &lt;- sum(is.na(dataframe[[column_name]]))\n  cat(\"Number of unique values: \", unique_count,\"\\nNumber of NA values: \", na_count,\"\\n\")\n}\n\n\nnumeric_column_analysis(\"idno\")\n\nFor Column:  idno \nRange: [ 1 , 300003000 ] \nAverage: 7231685  \nStandard Deviation:  35812998 \n\ncount_unique_na(\"idno\")\n\nIn Column:  idno \nNumber of unique values:  26199 \nNumber of NA values:  0 \n\nnumeric_column_analysis(\"year\")\n\nFor Column:  year \nRange: [ 2010 , 2010 ] \nAverage: 2010  \nStandard Deviation:  0 \n\ncount_unique_na(\"year\")\n\nIn Column:  year \nNumber of unique values:  1 \nNumber of NA values:  0 \n\nnumeric_column_analysis(\"age\")\n\nFor Column:  age \nRange: [ 14 , 101 ] \nAverage: 47.91529  \nStandard Deviation:  18.79573 \n\ncount_unique_na(\"age\")\n\nIn Column:  age \nNumber of unique values:  88 \nNumber of NA values:  137 \n\nnumeric_column_analysis(\"democ\")\n\nFor Column:  democ \nRange: [ 6 , 10 ] \nAverage: 9.452663  \nStandard Deviation:  1.043149 \n\ncount_unique_na(\"democ\")\n\nIn Column:  democ \nNumber of unique values:  6 \nNumber of NA values:  4451"
  },
  {
    "objectID": "challenge_3_Fall23.html#part-3.-practicing-string-functions-with-babynames",
    "href": "challenge_3_Fall23.html#part-3.-practicing-string-functions-with-babynames",
    "title": "Challenge_3: Joining Relational Data, Writing Your Own Functions, and String Operations",
    "section": "Part 3. Practicing String Functions with Babynames",
    "text": "Part 3. Practicing String Functions with Babynames\n\nImport the babynames data:\n\n\n#Type your code here\nbabynames &lt;- read.csv(\"./challenge3_data/babynames.csv\")\n\ndim(babynames)\n\n[1] 2084710       4\n\nhead(babynames)\n\n\n\n  \n\n\n\n\nUse different stirng functions to answer the following questions:\n(1) Find the longest name using count() and a string function.\nAnswer: We can observe from the below that there are multiple longest names with a length of 15 characters. They are ordered based on their frequency.\nThe longest name with highest frequency is : “Christopherjohn”.\n\n# Finding the longest name\nbabynames %&gt;%\n  mutate(LengthOfName = str_length(Name)) %&gt;%\n  count(Name, LengthOfName, sort = TRUE) %&gt;%\n  arrange(desc(LengthOfName))\n\n\n\n  \n\n\n\n(2) Use a string function to detect if the following names are present in the data:\n“Ronaldo”, “Messi”, “Wayne”, “Clarck”, “Rick”, and “Morty”.\nAnswer:\n\n# Checking the presence of names using string functions and custom functions:\nCheckPresence &lt;- function(name){\n  count &lt;- sum(str_detect(babynames$Name, name ) & str_length(babynames$Name) == str_length(name))\n  if(count == 0)\n    cat(name,\"is not present in the data\\n\")\n  else\n    cat(name,\"is present in the data\\n\")\n}\nCheckPresence(\"Ronaldo\")\n\nRonaldo is present in the data\n\nCheckPresence(\"Messi\")\n\nMessi is present in the data\n\nCheckPresence(\"Wayne\")\n\nWayne is present in the data\n\nCheckPresence(\"Clarck\")\n\nClarck is not present in the data\n\nCheckPresence(\"Rick\")\n\nRick is present in the data\n\nCheckPresence(\"Morty\")\n\nMorty is present in the data\n\n\n(3) Create a column LastName with just one value, “LastName”. Next, create another column FullName, by combing the strings of columns name and LastName, separating by a period. For example, a value in this new column should be like “Jacky.LastName”.\n\n#3\nbabynames &lt;- babynames %&gt;%\n  mutate(LastName = \"LastName\") %&gt;%\n  mutate(FullName = str_c(Name,LastName, sep = \".\"))\n\nhead(babynames)\n\n\n\n  \n\n\n\n(4) Find all “Elizabeth” in the data and replace “Elizabeth” with “Liz”.\nAnswer:\nAssumption1:\nReplaced the name “Elizabeth” with “Liz” using string functions. we keep ignore_case as FALSE, meaning matching is case sensitive. We can make it true to make the matching case insensitive.\nThe 4th row used to have “Elizabeth” and now it is changed to “Liz” - can be observed from below:\n\n#4 Assumption1\nbabynames_1 &lt;- babynames %&gt;%\n  mutate(Name = case_when(\n    str_detect(Name, regex(\"Elizabeth\", ignore_case=FALSE)) & str_length(Name) == str_length(\"Elizabeth\") ~ \"Liz\",\n    TRUE ~ Name\n  ), FullName=str_c(Name,LastName, sep = \".\"))\n\nhead(babynames_1)\n\n\n\n  \n\n\n\nassumption2:\nIn the first assumption we only changed Elizabeth to Liz when the whole name matches Elizabeth that is we did not change if we found Elizabeth in the middle of a Name like Elizabetha. In this assumption, we changed every occurrence of Elizabeth even if it is found in the middle of another name. Also we kept ignore_case as true meaning matching will be case insensitive. Because of this the string “elizabeth” will completely be removed from the data.\n\n#4 Assumption2\nbabynames_2 &lt;- babynames %&gt;%\n  mutate(Name = str_replace(Name, regex(\"Elizabeth\", ignore_case=TRUE), \"Liz\"),\n         FullName = str_replace(FullName, regex(\"Elizabeth\", ignore_case=TRUE), \"Liz\"))\n\nhead(babynames_2)"
  },
  {
    "objectID": "challenge_3_Fall23.html#part-4.-clean-data-with-import-and-string-functions",
    "href": "challenge_3_Fall23.html#part-4.-clean-data-with-import-and-string-functions",
    "title": "Challenge_3: Joining Relational Data, Writing Your Own Functions, and String Operations",
    "section": "Part 4. Clean data with import and string functions",
    "text": "Part 4. Clean data with import and string functions\nAs mentioned in the last Challenge, the original version of the survey on attitudes toward Same-Sex Marriage in Australia is raw and untidy data. You can open it in Excel and take a look at it.\nThe provided table includes estimates of the proportion of citizens choosing each of the four options, aggregated by Federal Electoral District, which are nested within one of 8 overarching Electoral Divisions.\nIn this case, we are going to identify the desired structure early in the process because clever naming of variables makes it much easier for later analysis. We will skip reading in redundant data (proportions and “totals” columns), and then can identify four potentially distinct pieces of information. Three grouping variables: Division (in column 1), District (also in column 1), and citizen Response (yes, no, unclear, and non-response), plus one value: aggregated response Count.\nThe ultimate goal is to use all the import and string functions we learned to generate data that looks like the data austrlia_data.csv we used in Challenge#2.\nThe data cleaning process should follow the following two steps. (Tips: some functions you will be using: mutate(),starts_with(), str_detect(), str_starts()) str_ends(), str_detect(), fill()).\n\nRead in data, skipping unneeded columns and renaming variables.\n\n#Reading the data from excel \naus_data &lt;- read_excel(\"./challenge3_data/australian_marriage_law_postal_survey_2017_-_response_final.xls\", sheet = \"Table 2\", skip = 6)\n\n\n#renaming columns and selecting only those columns\naus_data &lt;- aus_data %&gt;%\n  rename(\"Divison_District\" = 1, \"Yes\"=2, \"No\"=4, \"Illegible\" = 11, `No Response`=13) %&gt;%\n  select(Divison_District, Yes, No, Illegible, `No Response`)\n\n\n#removing empty rows and rows with District_Divison=NA, also removing the last 7 rows\naus_data &lt;- aus_data %&gt;%\n  filter(!is.na(Divison_District) & !str_detect(Divison_District, regex(\"Total\", ignore_case=TRUE) )) %&gt;%\n  filter(row_number() &lt;= n()-7 )\n\ncolnames(aus_data)\n\n[1] \"Divison_District\" \"Yes\"              \"No\"               \"Illegible\"       \n[5] \"No Response\"     \n\n\nCreate Division and District variables using separate() and fill(). You will also use string functions to help you.\nThe data is cleaned to look like the data given in challenge 2.\n\n# Separating the Division_District column\naus_data &lt;- aus_data %&gt;%\n  separate(Divison_District, into=c(\"Division\", \"District\"), sep=\"(?&lt;=Divisions)\", extra=\"drop\", fill=\"left\")\n\n# filling the Division column - default fill will fill all the empty rows until a new values comes\naus_data &lt;- aus_data %&gt;%\n  fill(Division)\n\n# removing rows with empty/NA District values and rearranging the columns to match the data from challenge 2\naus_data &lt;- aus_data %&gt;%\n  filter(District != \"\" & !is.na(District)) %&gt;%\n  select(2:6,1)\n\ndim(aus_data)\n\n[1] 150   6\n\nhead(aus_data)"
  },
  {
    "objectID": "challenge_1_Fall23_updated.html",
    "href": "challenge_1_Fall23_updated.html",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "",
    "text": "Make sure you change the author’s name in the above YAML header."
  },
  {
    "objectID": "challenge_1_Fall23_updated.html#setup",
    "href": "challenge_1_Fall23_updated.html#setup",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Setup",
    "text": "Setup\nIf you have not installed the following packages, please install them before loading them.\n\n# To install the packages\n#install.packages(\"tidyverse\")\n#install.packages(\"haven\")\n\n\nlibrary(tidyverse)\nlibrary(haven) #for loading other datafiles (SAS, STATA, SPSS, etc.)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\nlibrary(readxl)\nlibrary(dplyr)"
  },
  {
    "objectID": "challenge_1_Fall23_updated.html#challenge-overview",
    "href": "challenge_1_Fall23_updated.html#challenge-overview",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nThis first weekly challenge aims to practice the following skill sets: 1. Read datasets in different file types; 2. Describe the datasets; 3. Exploring a few basic functions of data transformation and wrangling and present some descriptive statistics (such as min, max, and median).\nThere will be coding components (reading datasets and data transformation) and writing components (describing the datasets and some statistical information). Please read the instructions for each part and complete your challenges."
  },
  {
    "objectID": "challenge_1_Fall23_updated.html#create-your-r-quarto-project-and-submit-the-standalone-.html-file.",
    "href": "challenge_1_Fall23_updated.html#create-your-r-quarto-project-and-submit-the-standalone-.html-file.",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Create your R quarto project and submit the standalone .html file.",
    "text": "Create your R quarto project and submit the standalone .html file.\nThis will be demonstrated in Sep 20 and 21 lab meetings."
  },
  {
    "objectID": "challenge_1_Fall23_updated.html#datasets",
    "href": "challenge_1_Fall23_updated.html#datasets",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Datasets",
    "text": "Datasets\nThere are four datasets provided in this challenge. Please download the following dataset files from Canvas or Google Classroom and save them to a folder within your project working directory (i.e.: “yourworkingdiectory_data”). If you don’t have a folder to store the datasets, please create one.\n\nbabynames.csv (Required) ⭐\nESS_5.dta (Option 1) ⭐\np5v2018.sav (Option 2)⭐\nrailroad.xlsx (Required)⭐⭐\n\nFind the _data folder, then use the correct R command to read the datasets."
  },
  {
    "objectID": "challenge_1_Fall23_updated.html#part-1required.-the-baby-names-dataset",
    "href": "challenge_1_Fall23_updated.html#part-1required.-the-baby-names-dataset",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Part 1(Required). The Baby Names Dataset",
    "text": "Part 1(Required). The Baby Names Dataset\n\nRead the dataset “babynames.csv”:\n\n\n#Type your code here\nbabynames = read.csv(\"./challenge1_data/babynames.csv\")\n\n\nData Description: Please use the necessary commands and codes and briefly describe this data with a short writing paragraph answering the following questions.\n\n#Type your code here; and write a paragraph answering the questions.\n# dimension of data:\ndim(babynames)\n\n[1] 2084710       4\n\n# To see the first few rows of the data to understand what the data is about\nhead(babynames)\n\n\n\n  \n\n\n\n(1) What is the dimension of the data (# of rows and columns)?\nAnswer: The data has 2084710 rows and 4 columns.\n(2) What do the rows and columns mean in this data?\nAnswer: The data has 4 columns - name of the baby, sex of the baby, number of occurrences of the name and the year of observation.\nBasically the data gives information about - in a particular year, how many babies have the same name for a particular gender, that is they show this count for male and female babies separately.\n(3) What is the unit of observation? In other words, what does each case mean in this data?\nAnswer: Unit of Observation is a row/observation representing a unique case of study in the data table. In this table each row is a case and it tells us information about number of same gender babies that have the same name in a given year. For example first row tells us that - in year 1880, there are 7065 female babies with the name “Mary”.\n(4) According to the lecture, is this a “tidy” data?\nAnswer: Yes. From the lecture we know that - Tidy data is a concept to describe data with a consistent form and clean structure with how it is stored. A tidy data is generally ready to be analyzed and visualized for many basic models. Coming to given data, we can observe that it has consistent and clean structure. Each row representing an observation/case and each column in the data table represent a variable giving particular information about the cases. So we can say that the given data “babynames” is tidy data.\nData Transformation: use necessary commands and codes and answer the following questions.\n\n#Type your code here; and write a paragraph answering the questions.\n\n# unique names - male, female, all:\n# we can use unique or distinct to calculate the unique names \n# using unique\nlength(unique(babynames[babynames$Sex == \"Male\", ]$Name))\n\n[1] 43653\n\nlength(unique(babynames[babynames$Sex == \"Female\", ]$Name))\n\n[1] 70225\n\nlength(unique(babynames$Name))\n\n[1] 102447\n\n#using distinct\n#nrow(distinct(filter(babynames, Sex==\"Male\"), Name))\n#nrow(distinct(filter(babynames, Sex==\"Female\"), Name))\n#nrow(distinct(babynames, Name))\n\n\n# number of years of data:\nlength(unique(babynames$Year))\n\n[1] 143\n\n#nrow(distinct(babynames, Year))\n\n\n# summarizing Occurrence column\nbabynames %&gt;% summarize(min_occ=min(Occurrences), mean_occ=mean(Occurrences), median_occ=median(Occurrences), max_occ=max(Occurrences))\n\n\n\n  \n\n\n# summarizing Occurrence column by decade\nbabynames %&gt;% \n  summarise(Occ=Occurrences, Decade=Year-(Year%%10)) %&gt;%\n  group_by(Decade) %&gt;%\n  summarise(Occurances_decade=sum(Occ)) %&gt;%\n  summarise(min_occ_decade=min(Occurances_decade), mean_occ_decade=mean(Occurances_decade), median_occ_decade=median(Occurances_decade), max_occ_decade=max(Occurances_decade))\n\n\n\n  \n\n\n\n(1) How many unique male names, unique female names, and total unique names are in the data?\nAnswer:\nMale unique names = 43653\nFemale unique names = 70225\nTotal unique names = 102447\n(2) How many years of names does this data record?\nAnswer: 143 years of data recorded.\n(3) Summarize the min, mean, median, and max of “Occurrence”. (Must use summarize())\nAnswer: min = 5, mean = 175.2112, median = 12, max = 99693\n(4) (Optional) Summarize the min, mean, median, and max of “Occurrence” by decade.\nAnswer: min = 2408032, mean = 24350971, median = 29368645, max = 39440656"
  },
  {
    "objectID": "challenge_1_Fall23_updated.html#part-2.-choose-one-option-of-tasks-to-complete",
    "href": "challenge_1_Fall23_updated.html#part-2.-choose-one-option-of-tasks-to-complete",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Part 2. Choose One Option of Tasks to Complete",
    "text": "Part 2. Choose One Option of Tasks to Complete\nIn this part, please choose either of the two datasets to complete the tasks."
  },
  {
    "objectID": "challenge_1_Fall23_updated.html#optional-1-the-european-social-survey-dataset",
    "href": "challenge_1_Fall23_updated.html#optional-1-the-european-social-survey-dataset",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Optional 1: The European Social Survey Dataset",
    "text": "Optional 1: The European Social Survey Dataset\nThe European Social Survey (ESS) is an academically-driven multi-country survey, which has been administered in over 30 countries to date. Its three aims are, firstly - to monitor and interpret changing public attitudes and values within Europe and to investigate how they interact with Europe’s changing institutions, secondly - to advance and consolidate improved methods of cross-national survey measurement in Europe and beyond, and thirdly - to develop a series of European social indicators, including attitudinal indicators.\nIn the fifth round, the survey covers 28 countries and investigates two major topics: Family Work and Wellbeing and Justice.\n\nRead the dataset “ESS_5.dta”.\n\n#Type your code here\ness5_data_full = read_dta(\"./challenge1_data/ESS_5.dta\")\n\n\n\n\nData Description: Please use the necessary commands and codes and briefly describe this data with a short writing paragraph answering the following questions.\n(1) What is the dimension of the data (# of rows and columns)?\nAnswer: The data contains - 52458 rows and 696 columns\n\n#Type your code here; and write a paragraph answering the questions.\ndim(ess5_data_full)\n\n[1] 52458   696\n\nhead(ess5_data_full)\n\n\n\n  \n\n\n\nAs we can see, this data is very large. We don’t want to study the whole data. Let’s just reload the following selected columns: “idno, essroud, male, age, edu, income_10, eth_major, media (a standardized measure of the frequency of media consumption), and cntry”.\n\n#Type your code here; and write a paragraph answering the questions.\ness5_data &lt;- select(ess5_data_full, idno, essround, male, age, edu, income_10, eth_major, media, cntry)\n\n#dimension of reloaded data\ndim(ess5_data)\n\n[1] 52458     9\n\nhead(ess5_data)\n\n\n\n  \n\n\n\n\n\n\nFor the reloaded data, what do the rows and columns mean in this data?\nAnswer: The new data has 52458 rows and 9 columns.\nEach row corresponds to an individual’s survey report. And each column contains the information about the candidate collected in the survey. idno - ID given to that candidate, essround - round of survey - given 5th, male - code for gender of candidate - 0-female : 1-male , age - age of the candidate, edu - code for education level of candidate, income_10 - code for household’s net income, eth_major - whether the candidate belongs to minority ethnic group in the country, media - a standardized measure of the frequency of a candidate’s media consumption, cntry - country the candidate belongs to.\n\n(3) What is the unit of observation? In other words, what does each case mean in this data?\nAnswer: Each row is a case/observation. It gives information about a particular candidate’s survey report like id, gender, education level, income, media utilization, country and other information relavant to the survey.\n(4) According to the lecture, is this a “tidy” data?\nAnswer: Yes. The data is well structured and clear. Each row represents an individual’s report and each column represents a variable, giving information about the candidate like id, age, country etc. Even though the data has NA values we can still call it tidy data.\nData Transformation: use necessary commands and codes and answer the following questions.\n\n#Type your code here; and write a paragraph answering the questions.\n# Here I am considering only the partial data that we created above and not the complete data \n\n# unique countries\nlength(unique(ess5_data$cntry))\n\n[1] 27\n\n#summarise data\ness5_data %&gt;% summarise(min_age=min(age, na.rm = T), max_age=max(age, na.rm = T), average_age=mean(age, na.rm = T), min_edu=min(edu, na.rm = T), max_edu=max(edu, na.rm = T), average_edu=mean(edu, na.rm = T), min_media=min(media, na.rm = T), max_media=max(media, na.rm = T), average_media=mean(media, na.rm = T))\n\n\n\n  \n\n\n# eth_major and income_10 NA values count:\ncolSums(is.na(select(ess5_data, eth_major)))\n\neth_major \n     1310 \n\ncolSums(is.na(select(ess5_data, income_10)))\n\nincome_10 \n    12620 \n\n\n(1) How many unique countries are in the data?\nAnswer: There are 27 unique countries in the reloaded data.\n(2) What are the range and average of the following variables: “age”, “edu”, and “media”? Must use summarize().\nAnswer: we are using na.rm = T to remove the NA values from our calculations\nage range = [14, 101], average = 47.91529\nedu range = [1, 4], average = 2.767531\nmedia range = [0, 1], average = 0.4786802\n(3) How many missing data (NA) are in the following variables: “eth_major” and “income_10”? (tips: use is.na())\nAnswer:\nIn eth_major column there are 1310 entries with NA values\nIn income_10 column there are 12620 entries with NA values"
  },
  {
    "objectID": "challenge_1_Fall23_updated.html#optional-2-polity-v-data",
    "href": "challenge_1_Fall23_updated.html#optional-2-polity-v-data",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Optional 2: Polity V Data",
    "text": "Optional 2: Polity V Data\nThe Polity data series is a data series in political science research. Polity is among prominent datasets that measure democracy and autocracy. The Polity5 dataset covers all major, independent states in the global system over the period 1800-2018 (i.e., states with a total population of 500,000 or more in the most recent year; currently 167 countries with Polity5 refinements completed for about half those countries).\n\nRead the dataset “p5v2018.sav”.\n\n#Type your code here\npolityv_data_full &lt;- read_sav(\"./challenge1_data/p5v2018.sav\")\n\n\n\n\nData Description: Please use the necessary commands and codes and briefly describe this data with a short writing paragraph answering the following questions.\n\n#Type your code here; and write a paragraph answering the questions.\ndim(polityv_data_full)\n\n[1] 17574    37\n\nhead(polityv_data_full)\n\n\n\n  \n\n\n\n(1) What is the dimension of the data (# of rows and columns)?\nAnswer: There are 17574 rows and 37 columns in the data.\nAs we can see, this data contains many columns. We don’t want to study the whole data. Let’s keep the first seven columns and the ninth and ten columns.\n\n#Type your code here; and write a paragraph answering the questions.\npolityv_data &lt;- subset(polityv_data_full, select = c(1:7,9, 10) )\n\n(2) For the reloaded data, what do the rows mean in this data? What do the columns (#2-#8) mean? (If you have questions, check out p.11-16 of the User Manual/Codebook of the dataset).\nAnswer: The data has 17574 rows and 9 columns.\nEach row gives information about a country’s democracy and autocracy measures in a particular year.\nThe columns gives information about the country, the year of record, and measures. cyear - country year: a unique identifier for each country year, ccode - a country’s assigned numeric code, scode - a country’s assigned alpha code, country - name of the country, year - year of record, flag - confidence of measures, democ - the democracy index/score, autoc - the autocracy index/code.\n(3) What is the unit of observation? In other words, what does each case mean in this data?\nAnswer: Each row in the data is a case/observation. It gives information about a country’s democracy and autocracy measures in a particular year.\n(4) According to the lecture, is this a “tidy” data?\nAnswer: Yes, the data is tidy data since it is well structured and clear. Each row is a case giving information about a country’s democracy and autocracy measures and each column is a variable representing some information about the country, the year of record, measures etc.\n\nData Transformation: use necessary commands and codes and answer the following questions.\n\n#Type your code here; and write a paragraph answering the questions.\n# unique countries\nlength(unique(polityv_data$country))\n\n[1] 195\n\n# unique years\nlength(unique(polityv_data$year))\n\n[1] 245\n\n# range and average of democ and autoc\npolityv_data %&gt;% \n  filter(democ &gt;= 0) %&gt;%\n  summarise(democ_min=min(democ), democ_max=max(democ), democ_mean=mean(democ))\n\n\n\n  \n\n\npolityv_data %&gt;% \n  filter(autoc &gt;= 0) %&gt;%\n  summarise(autoc_min=min(autoc), autoc_max=max(autoc), autoc_mean=mean(autoc))\n\n\n\n  \n\n\n# democ and autoc NA values count:\ncolSums(is.na(select(polityv_data, democ)))\n\ndemoc \n    0 \n\ncolSums(is.na(select(polityv_data, autoc)))\n\nautoc \n    0 \n\n# if we consider -88,-77,-66 as missing values too then the missing values count will be as follows:\n#length(which(polityv_data$democ == -88 | polityv_data$democ == -77 |polityv_data$democ == -66))\n\n#length(which(polityv_data$autoc == -88 | polityv_data$autoc == -77 |polityv_data$autoc == -66))\n\n(1) How many unique countries are in the data?\nAnswer: 195\n(2) How many years does this data record?\nAnswer: 245\n(3) What are the range and average of the following variables: “democ” and “autoc”?\nAnswer:\nWe filter values &gt;= 0 and use them to find the required data. ( we are using &gt;=0 condition because it was clear from the data that all the values are &gt;=0 except for -88,-77,-66 so we can use this condition without the fear of losing any other important data.\ndemoc range: [0,10]\ndemoc average: 3.501163\nautoc range: [0,10]\nautoc average: 4.02195\n** Noted that in this data, negative integers (-88, -77, and -66) represent special cases. You should exclude them when calculating the range, average, and NAs.\n(4) How many missing data (NA) are in the following variables: “democ” and “autoc”? (tips: use is.na())\nNumber of NA values in democ: 0\nNumber of NA values in autoc : 0"
  },
  {
    "objectID": "challenge_1_Fall23_updated.html#part-3.-the-railroad-employee-data",
    "href": "challenge_1_Fall23_updated.html#part-3.-the-railroad-employee-data",
    "title": "Challenge_1: Data Import, Description, and Transformation(1)",
    "section": "Part 3. The Railroad Employee Data",
    "text": "Part 3. The Railroad Employee Data\n\nRead the dataset “railroads.xls”.\nMany government organizations still use Excel spreadsheets to store data. This railroad dataset, published by the Railroad Retirement Board, is a typical example. It records the number of employees in each county and state in 2012.\nPlease load the data in R in a clean manner. You can start by doing the following things step by step.\n(1) Read the first sheet of the Excel file;\n(2) Skipping the title rows;\n(3) Removing empty columns\n(4) Filtering “total” rows\n(5) Remove the table notes (the last two rows)\n\n#Type your code here\nlibrary(readxl)\nlibrary(dplyr)\n\n#1,2 loading the data and skipping the title rows\nrailroads_data &lt;- read_excel(\"./challenge1_data/railroads.xls\", sheet = 1, skip = 2)\n\n#3 removing the empty columns\nrailroads &lt;- railroads_data[-c(2,4)]\n\n# removing the empty rows\nrailroads &lt;- railroads[!apply(is.na(railroads) | railroads == \"\", 1, all),]\n\n#4 removing rows which contain \"Total\" in them\nrailroads &lt;- railroads %&gt;% filter(!grepl('Total', STATE))\n\n#5 removing the last 3 rows - table notes, can do my mentioning row numbers specifically or by using filter method\nrailroads &lt;- railroads %&gt;% filter(row_number() &lt;= n()-3)\n#railroads &lt;- railroads[-(2987:2990),] \n\n\n\n\nData Description: Please use the necessary commands and codes and briefly describe this data with a short writing paragraph answering the following questions.\n\n#Type your code here; and write a paragraph answering the questions.\n\n# dimension of data\ndim(railroads)\n\n[1] 2930    3\n\n# viewing initial few lines of data to understand what the data is about\nhead(railroads)\n\n\n\n  \n\n\n\n(1) What is the dimension of the data (# of rows and columns)?\nAnswer: Considering we removed all the empty rows, empty columns, rows with ‘Total’ in them, rows corresponding to table notes.\nThe cleaned data has 2930 rows and 3 columns.\n(2) What do the rows and columns mean?\nAnswer: There are 3 columns - State - gives alpha code of state, County - name of a county from the state and Total ( representing the number of employees). Each row gives us information about number of rail road employees in a particular county of a state.\n(3) What is the unit of observation? In other words, what does each case mean in this data?\nAnswer: In this data, each row is a case that is an unit of observation which gives us information about a particular county, the state it belongs to and the total number of rail road employees in that particular county.\n(4) According to the lecture, is this a “tidy” data?\nAnswer: Yes. The initial data is not clean, but once we made the necessary changes to the loaded data, the new data formed is a clean and structured data. Each row is an observation and each column is a variable giving some information about each observation. So yes, the modified data is a tidy data.\n\nData Transformation: use necessary commands and codes and answer the following questions.\n\n#Type your code here; and write a paragraph answering the questions.\n\n#1 unique states and counties\nlength(unique(railroads$STATE))\n\n[1] 53\n\nlength(unique(railroads$COUNTY))\n\n[1] 1709\n\n#2 total number of employees\nrailroads %&gt;%\n  summarise(total_emp = sum(TOTAL))\n\n\n\n  \n\n\n#3 min, max, mean and median of total_employees\nrailroads %&gt;% summarize(min=min(TOTAL), mean=mean(TOTAL), median=median(TOTAL), max=max(TOTAL))\n\n\n\n  \n\n\n#4 counties and states with most employees: here list given in desc order \narrange(railroads, desc(TOTAL))\n\n\n\n  \n\n\nrailroads %&gt;% \n  group_by(STATE) %&gt;%\n  summarise(sumTotal=sum(TOTAL)) %&gt;%\n  arrange(desc(sumTotal), .by_group=TRUE)\n\n\n\n  \n\n\n\n(1) How many unique counties and states are in the data? (tips: you can try using the across() function to do an operation on two columns at the same time)\nAnswer:\nNumber of unique states = 53\nNumber of unique counties = 1709\n(2) What is the total number of employees (total_employees) in this data?\nAnswer: total number of employees in this data = 255432\n(3) What are the min, max, mean, and median of “total_employees”\nAnswer:\nmin = 1\nmean = 87.17816\nmedian = 21\nmax = 8207\n(4) Which states have the most employees? And which countries have the most employees? (tips: use group_by() and arrange())\nAnswer:\nTop 5 Counties with the most employees = “COOK” - IL, “TARRANT”-TX, “DOUGLAS”-NE, “SUFFOLK”-NY, “INDEPENDENT CITY”-VA.\nTop 5 States with the most employees = “TX”, “IL”, “NY”, “NE”, “CA”."
  },
  {
    "objectID": "challenge_2_Fall23_updated.html",
    "href": "challenge_2_Fall23_updated.html",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "",
    "text": "Make sure you change the author’s name in the above YAML header."
  },
  {
    "objectID": "challenge_2_Fall23_updated.html#setup",
    "href": "challenge_2_Fall23_updated.html#setup",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "Setup",
    "text": "Setup\nIf you have not installed the following packages, please install them before loading them.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(haven) #for loading other datafiles (SAS, STATA, SPSS, etc.)\nlibrary(stringr) # if you have not installed this package, please install it.\nlibrary(lubridate)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "challenge_2_Fall23_updated.html#challenge-overview",
    "href": "challenge_2_Fall23_updated.html#challenge-overview",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nBuilding on the lectures in week#3 and week#4, we will continually practice the skills of different transformation functions with Challenge_2. In addition, we will explore the data more by conducting practices with pivoting data and dealing with date-time data.\nThere will be coding components and writing components. Please read the instructions for each part and complete your challenges."
  },
  {
    "objectID": "challenge_2_Fall23_updated.html#datasets",
    "href": "challenge_2_Fall23_updated.html#datasets",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "Datasets",
    "text": "Datasets\nThere are four datasets provided in this challenge. Please download the following dataset files from Canvas or Google Classroom and save them to a folder within your project working directory (i.e.: “yourworkingdiectory_data”). If you don’t have a folder to store the datasets, please create one.\n\nESS_5.dta (Part 1) ⭐\np5v2018.sav (Part 1)⭐\naustrlian_data.csv (Part 3)⭐\nFedFundsRate.csv (Part 4)⭐\n\nFind the _data folder, then use the correct R command to read the datasets."
  },
  {
    "objectID": "challenge_2_Fall23_updated.html#part-1required.-depending-on-the-data-you-chose-in-challenge1-ess_5-or-polity-v-please-use-that-data-to-complete-the-following-tasks",
    "href": "challenge_2_Fall23_updated.html#part-1required.-depending-on-the-data-you-chose-in-challenge1-ess_5-or-polity-v-please-use-that-data-to-complete-the-following-tasks",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "Part 1(Required). Depending on the data you chose in Challenge#1 (ESS_5 or Polity V), please use that data to complete the following tasks",
    "text": "Part 1(Required). Depending on the data you chose in Challenge#1 (ESS_5 or Polity V), please use that data to complete the following tasks"
  },
  {
    "objectID": "challenge_2_Fall23_updated.html#if-you-are-using-the-ess_5-data",
    "href": "challenge_2_Fall23_updated.html#if-you-are-using-the-ess_5-data",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "If you are using the ESS_5 Data:",
    "text": "If you are using the ESS_5 Data:\n\nRead the dataset and keep the first 39 columns.\n\n\n#Type your code here\ness_data_org &lt;- read_dta(\"./challenge2_data/ESS_5.dta\") %&gt;%  \n  select(1:39)\n\ndim(ess_data_org)\n\n[1] 52458    39\n\n\n\nConduct the following transformation for the data by using mutate() and other related functions :\n(1) Create a new column named “YearOfBirth” using the information in the “age” column.\n\n#Type your code here\n#1 Creating \"YearOfBirth\" from age column\ncurr_year &lt;- year(Sys.Date())\ness_data_mod &lt;- ess_data_org %&gt;%\n  mutate(YearOfBirth = curr_year - age)\n\n\n\n\nCreate a new column named “adult” using the information in the “age” column.\n\n\n#2 Creating \"adult\" from age column\ness_data_mod &lt;- ess_data_mod %&gt;%\n  mutate(adult = ifelse(age &gt;= 18, \"Yes\", \"No\"))\n\n\n\n\nRecode the “commonlaw” column: if the value is 0, recode it as “non-common-law”; if the value is 1, recode it as “common-law”.\n\n\n#3 Recoding \"commonlaw\" column\ness_data_mod &lt;- ess_data_mod %&gt;%\n  mutate(commonlaw = ifelse(commonlaw == 0, \"non-common-law\", \"common-law\"))\n\n\n\n\nRecode the “vote” column: if the value is 3, recode it as 1; if the value is smaller than 3, recode it as 0. Make sure to exclude the NAs.\n\n\n#4 Recoding \"vote\" column\n#unique(ess_data_mod$vote)\n#typeof(ess_data_mod$vote)\n\ness_data_mod &lt;- ess_data_mod %&gt;%\n  mutate(vote = case_when(\n    !is.na(vote) & vote == 3 ~ 1,\n    !is.na(vote) & vote &lt; 3 ~ 0,\n    TRUE ~ NA\n  ))\n\n\n\n\nMove the column “YearOfBirth”, “adult,” “commonlaw” and “vote” right after the “essround” column (the 2nd column in order).\n\n\n#5 Moving columns\ness_data_mod &lt;- ess_data_mod %&gt;%\n  select(1:2, YearOfBirth, adult, commonlaw, vote, everything())\n\n\n\n\nAnswer the question: What is the data type of the “commonlaw” column before and after recoding? And what is the data type of the “vote” column before and after recoding?\n\n\n#6 Data type before and after recoding\ncat(\"commonlaw data type before recoding: \", typeof(ess_data_org$commonlaw), \"\\n\")\n\ncommonlaw data type before recoding:  double \n\ncat(\"commonlaw data type after recoding: \", typeof(ess_data_mod$commonlaw), \"\\n\")\n\ncommonlaw data type after recoding:  character \n\ncat(\"vote data type before recoding: \", typeof(ess_data_org$vote), \"\\n\")\n\nvote data type before recoding:  double \n\ncat(\"vote data type after recoding: \", typeof(ess_data_mod$vote), \"\\n\")\n\nvote data type after recoding:  double"
  },
  {
    "objectID": "challenge_2_Fall23_updated.html#if-you-are-using-the-polity-v-data---not-using-this.",
    "href": "challenge_2_Fall23_updated.html#if-you-are-using-the-polity-v-data---not-using-this.",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "** If you are using the Polity V Data: - Not using this.**",
    "text": "** If you are using the Polity V Data: - Not using this.**\n\nRead the dataset and keep the first 11 columns.\n\n\n#Type your code here\n\n\nConduct the following transformation for the data by using mutate() and other related functions :\n(1) Create a new column named “North America” using the information in the “country” column. Note: “United States,” “Mexico,” or “Canada” are the countries in North America. In the new “North America” column, if a country is one of the above three countries, it should be coded as 1, otherwise as 0.\n(2) Recode the “democ” column: if the value is 10, recode it as “Well-Functioning Democracy”; if the value is greater than 0 and smaller than 10, recode it as “Either-Autocracy-or-Democracy”; if the value is 0, recode it as “Non-democracy”; if the value is one of the following negative integers (-88, -77, and -66), recode it as “Special-Cases.”\n(3) Move the column “North America” and “democ” right before the “year” column (the 6th column in order).\n(4) Answer the question: What is the data type of the “North America” column? What is the data type of the “democ” column before and after recoding?\n\n\n#Type your code here"
  },
  {
    "objectID": "challenge_2_Fall23_updated.html#part-2.-generate-your-own-data",
    "href": "challenge_2_Fall23_updated.html#part-2.-generate-your-own-data",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "Part 2. Generate your own Data",
    "text": "Part 2. Generate your own Data\n\nGenerate an untidy data that includes 10 rows and 10 columns. In this dataset, column names are not names of variables but a value of a variable.\n*Note: do not ask ChatGPT to generate a dataframe for you. I have already checked the possible questions and answers generated by AI.\n\n\nactors_data_untidy &lt;- data.frame(\n  Year = 2011:2020,\n  `Adam_Sandler` = c(4, 3, 1, 4, 4, 1, 3, 4, 2, 4),\n  `Rock` = c(1, 1, 5, 1, 3, 2, 4, 2, 4, 0),\n  `Robert_Downey_Jr` = c(1, 1, 1, 2, 1, 1, 1, 1, 1, 1),\n  `Chris_Evans` = c(3, 2, 2, 2, 3, 1, 2, 1, 4, 0),\n  `Scarlett_Johansson` = c(2, 2, 3, 3, 1, 4, 2, 2, 4, 0),\n  `Kevin_Hart` = c(3, 3, 3, 5, 2, 4, 3, 1, 3, 0),\n  `Chris_Hemsworth` = c(1, 4, 3, 0, 4, 4, 2, 3, 3, 1),\n  `Jennifer_Lawrence` = c(3, 3, 3, 3, 2, 3, 1, 1, 1, 0),\n  `Emma_Watson` = c(2, 1, 1, 1, 2, 0, 2, 0, 1, 0)\n  #`Will_Smith` = c(0, 1, 2, 1, 2, 2, 1, 0, 5, 1)\n)\n\nprint(actors_data_untidy)\n\n   Year Adam_Sandler Rock Robert_Downey_Jr Chris_Evans Scarlett_Johansson\n1  2011            4    1                1           3                  2\n2  2012            3    1                1           2                  2\n3  2013            1    5                1           2                  3\n4  2014            4    1                2           2                  3\n5  2015            4    3                1           3                  1\n6  2016            1    2                1           1                  4\n7  2017            3    4                1           2                  2\n8  2018            4    2                1           1                  2\n9  2019            2    4                1           4                  4\n10 2020            4    0                1           0                  0\n   Kevin_Hart Chris_Hemsworth Jennifer_Lawrence Emma_Watson\n1           3               1                 3           2\n2           3               4                 3           1\n3           3               3                 3           1\n4           5               0                 3           1\n5           2               4                 2           2\n6           4               4                 3           0\n7           3               2                 1           2\n8           1               3                 1           0\n9           3               3                 1           1\n10          0               1                 0           0\n\ndim(actors_data_untidy)\n\n[1] 10 10\n\n\n\nUse the correct pivot command to convert the data to tidy data.\n\n\n#Type your code here\nactors_data_tidy &lt;- actors_data_untidy %&gt;%\n  pivot_longer(cols = -Year, names_to = \"Actor\", values_to = \"movies_count\")\n\nprint(actors_data_tidy)\n\n# A tibble: 90 × 3\n    Year Actor              movies_count\n   &lt;int&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1  2011 Adam_Sandler                  4\n 2  2011 Rock                          1\n 3  2011 Robert_Downey_Jr              1\n 4  2011 Chris_Evans                   3\n 5  2011 Scarlett_Johansson            2\n 6  2011 Kevin_Hart                    3\n 7  2011 Chris_Hemsworth               1\n 8  2011 Jennifer_Lawrence             3\n 9  2011 Emma_Watson                   2\n10  2012 Adam_Sandler                  3\n# ℹ 80 more rows\n\ndim(actors_data_tidy)\n\n[1] 90  3\n\n\n\nGenerate an untidy data that includes 10 rows and 5 columns. In this dataset, an observation is scattered across multiple rows.\n\n\n#Type your code here\nstudent_data_untidy &lt;- data.frame(\n  student_id = c(2016001,2016001,2016002,2016002,2016003,2016003,2016004,2016004,2016005,2016005),\n    year = c(2016,2016,2016,2016,2016,2016,2016,2016,2016,2016),\n  student_name = c(\"Rick\", \"Rick\", \"Andy\", \"Andy\", \"James\", \"James\", \"Ryan\", \"Ryan\", \"Evan\", \"Evan\"),\n  category = c(\"grade\", \"attandance\", \"grade\", \"attandance\", \"grade\", \"attandance\", \"grade\", \"attandance\", \"grade\", \"attandance\" ),\n  value = c(\"A+\",95,\"A\",80,\"B\",91,\"B+\",86,\"A-\",96) \n)\n\nprint(student_data_untidy)\n\n   student_id year student_name   category value\n1     2016001 2016         Rick      grade    A+\n2     2016001 2016         Rick attandance    95\n3     2016002 2016         Andy      grade     A\n4     2016002 2016         Andy attandance    80\n5     2016003 2016        James      grade     B\n6     2016003 2016        James attandance    91\n7     2016004 2016         Ryan      grade    B+\n8     2016004 2016         Ryan attandance    86\n9     2016005 2016         Evan      grade    A-\n10    2016005 2016         Evan attandance    96\n\ndim(student_data_untidy)\n\n[1] 10  5\n\n\n\nUse the correct pivot command to convert the data to tidy data.\n\n\n#Type your code here\nstudent_data_tidy &lt;- student_data_untidy %&gt;%\n  pivot_wider(names_from = category, values_from = value)\nstudent_data_tidy$attandance &lt;- as.numeric(student_data_tidy$attandance )\n\nprint(student_data_tidy)\n\n# A tibble: 5 × 5\n  student_id  year student_name grade attandance\n       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt;\n1    2016001  2016 Rick         A+            95\n2    2016002  2016 Andy         A             80\n3    2016003  2016 James        B             91\n4    2016004  2016 Ryan         B+            86\n5    2016005  2016 Evan         A-            96\n\ndim(student_data_tidy)\n\n[1] 5 5"
  },
  {
    "objectID": "challenge_2_Fall23_updated.html#part-3.-the-australian-data",
    "href": "challenge_2_Fall23_updated.html#part-3.-the-australian-data",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "Part 3. The Australian Data",
    "text": "Part 3. The Australian Data\nThis is another tabular data source published by the Australian Bureau of Statistics that requires a decent amount of cleaning. In 2017, Australia conducted a postal survey to gauge citizens’ opinions towards same sex marriage: “Should the law be changed to allow same-sex couples to marry?” All Australian citizens are required to vote in elections, so citizens could respond in one of four ways: vote yes, vote no, vote in an unclear way(illegible), or fail to vote. (See the “Explanatory Notes” sheet for more details.)\nI have already cleaned up the data for you and you can directly import it. We will come back to clean and process the original “messy” data after we learn some string functions in the later weeks.\n\nRead the dataset “australian_data.csv”:\n\n\n#Type your code here\naustralian_data_untidy &lt;- read.csv(\"./challenge2_data/australian_data.csv\", header=TRUE, row.names = 1, check.names = FALSE)\n\n\nData Description: Please use the necessary commands and codes and briefly describe this data with a short writing paragraph answering the following questions.\n\n#Type your code here\n#1 dimension of data\ndim(australian_data_untidy)\n\n[1] 150   6\n\n#2\nhead(australian_data_untidy)\n\n\n\n  \n\n\n\n(1) What is the dimension of the data (# of rows and columns)?\nAnswer: The data contains 150 rows and 6 columns. Dimension is 150x6.\n(2) What do the rows and columns mean in this data?\nAnswer: The column gives information about the following: District: name of the district, Yes: number of citizens voted Yes, No: number of citizens voted No, Illegible: number of citizen’s votes that are unclear, No Response: number of citizens that did not participate in voting, Division: the division that the district belongs to.\nEach row gives citizen’s voting information about a particular district in Australia that is number of people who voted yes and no, people whose vote was unclear to read, number of people who did not vote at all along with the division the district belongs to.\n(3) What is the unit of observation? In other words, what does each case mean in this data?\nAnswer: Here the unit of observation is a district in a given division. Each row is a case/observation which tells us about a particular district’s voting information.\n(4) According to the lecture, is this a “tidy” data? Why?\nAnswer: According to above assumption of unit of observation, the give data is tidy data. Note: while loading data we made sure that the first column that has row numbers is skipped so the loaded data is clean and structured.\n(5) If this is not a tidy data, please use the necessary commands to make it “tidy”.\nAnswer: To make this a different version of tidy data, we have to convert the columns “Yes”,“No”,“Illegible” and “No Response” to “vote_category” and “vote_count” columns.\n\naustralian_data &lt;- australian_data_untidy %&gt;%\n  gather(key=\"vote_category\",value=\"vote_count\", Yes, No, Illegible, `No Response`)\n\ndim(australian_data)\n\n[1] 600   4\n\nhead(australian_data)\n\n\n\n  \n\n\n\nData Transformation: use necessary commands and codes and answer the following questions. If you reshape the data in the previous step, please work on the reshaped data.\n\n#Type your code here\n#1\n#Number of Districts\nnrow(unique(australian_data[, c(\"District\", \"Division\")]))\n\n[1] 150\n\n#Number of divisions\nlength(unique(australian_data$Division))\n\n[1] 8\n\n#2 Creating new column \"district turnout(%)\"\naustralian_data &lt;- australian_data %&gt;%\n  group_by(District) %&gt;%\n  mutate(total_votes = sum(vote_count[vote_category %in% c(\"Yes\", \"No\", \"Illegible\")])) %&gt;%\n  mutate(total_population = sum(vote_count)) %&gt;%\n  mutate(district_turnout = (total_votes / total_population) * 100) %&gt;%\n  ungroup()\n\n#3 \n\n#Number of supporters and opposers\naustralian_data %&gt;% \n  filter(vote_category == \"Yes\") %&gt;%\n  summarise(`Number of Supporters` = sum(vote_count))\n\n\n\n  \n\n\naustralian_data %&gt;% \n  filter(vote_category == \"No\") %&gt;%\n  summarise(`Number of Opposers` = sum(vote_count))\n\n\n\n  \n\n\n# District with maximum supporters\naustralian_data %&gt;% \n  filter(vote_category == \"Yes\") %&gt;%\n  summarise(`District with maximum supporters` = District[which.max(vote_count)], \"votes_count\"=max(vote_count))\n\n\n\n  \n\n\n# Division with highest yes %\naustralian_data %&gt;% \n  group_by(Division) %&gt;%\n  summarise(Division_approval_rate= sum(vote_count[vote_category==\"Yes\"])*100/(sum(vote_count[vote_category %in% c(\"Yes\",\"No\",\"Illegible\")]))) %&gt;%\n  summarise(`Division with highest Yes %` = Division[which.max(Division_approval_rate)])\n\n\n\n  \n\n\n# Average approval rate at division level\n# Assumption 1\naustralian_data %&gt;% \n  group_by(Division) %&gt;%\n  summarise(Division_approval_rate= sum(vote_count[vote_category==\"Yes\"])*100/(sum(vote_count[vote_category %in% c(\"Yes\",\"No\",\"Illegible\")]))) %&gt;%\n  summarise(`average approval rate at Divison level` = mean(Division_approval_rate))\n\n\n\n  \n\n\n# Assumption 2\naustralian_data %&gt;% \n  group_by(District) %&gt;%\n  summarise(division=Division, approval_rate =sum(vote_count[vote_category==\"Yes\"])*100/(sum(vote_count[vote_category %in% c(\"Yes\",\"No\",\"Illegible\")]))  ) %&gt;%\n  group_by(division) %&gt;%\n  summarise(Avg_approval_rate = mean(approval_rate)) %&gt;%\n  arrange(desc(Avg_approval_rate), .by_group=TRUE)\n\n\n\n  \n\n\n\n(1) How many districts and divisions are in the data?\nAnswer:\nSince multiple districts can have same, we use the combination of district and division to find unique districts. This will give us correct number even if districts from different divisions have same name.\nNumber of districts: 150\nNumber of divisions: 8\n(2) Use mutate() to create a new column “district turnout(%)”. This column should be the voting turnout in a given district, or the proportion of people cast votes (yes, no, and illegible) in the total population of a district.\nAnswer: The command is shown above.\n(3) please use summarise() to estimate the following questions:\n\nIn total, how many people support same-sex marriage in Australia, and how many people oppose it?\nAnswer:\nNumber of Supporters = 7817247\nNumber of Opposers = 4873987\n\nWhich district has most people supporting the policy, and how many?\nAnswer:\nDistrict with most people supporting the policy: Canberra(d)\nNumber of supporters from that district: 89590\n\nWhich division has the highest approval rate (% of “yes” in the total casted votes)? And what is the average approval rate at the division level?\nAnswer:\nDivision with highest Yes % = Australian Capital Territory Divisions\n\nSince average approval at division level definition is not clearly stated, I found the following 2 set of values.\n1.Found approval rate for each division and then took the average for all the divisions, this value is 63.30475\n2.Finding approval rate for each district and then finding average of all the districts’ approval rates that belong to a given division. Those values are given below:\nAverage approval rate at division level:\nAustralian Capital Territory Divisions = 73.87287\nVictoria Divisions = 64.44427\nWestern Australia Divisions = 63.44548\nTasmania Divisions = 63.22453\nSouth Australia Divisions = 62.05639\nQueensland Divisions = 60.23834\nNorthern Territory Divisions = 59.69649\nNew South Wales Divisions = 57.07813"
  },
  {
    "objectID": "challenge_2_Fall23_updated.html#part-4.-the-marco-economic-data",
    "href": "challenge_2_Fall23_updated.html#part-4.-the-marco-economic-data",
    "title": "Challenge_2: Data Transformation(2), Pivot and Date-Time Data",
    "section": "Part 4. The Marco-economic Data",
    "text": "Part 4. The Marco-economic Data\nThis data set runs from July 1954 to March 2017, and includes daily macroeconomic indicators related to the effective federal funds rate - or the interest rate at which banks lend money to each other in order to meet mandated reserve requirements.\n\nRead the dataset “FedFundsRate.csv”:\n\n\n#Type your code here\nfed_rates_org &lt;- read.csv(\"./challenge2_data/FedFundsRate.csv\", header=TRUE, check.names = FALSE)\n\n\nData Description: Please use the necessary commands and codes and briefly describe this data with a short writing paragraph answering the following questions.\n\n#Type your code here\ndim(fed_rates_org)\n\n[1] 904  10\n\nhead(fed_rates_org)\n\n\n\n  \n\n\n\n(1) What is the dimension of the data (# of rows and columns)?\nAnswer: Data has 904 rows and 10 columns. Dimension is 904x10.\n(2) What do the rows and columns mean in this data?\nAnswer: The columns represent the following: Year,Month,Day - gives us the date for which the federal rates are being shown.\n“Federal Funds Target Rate” - represents the target rate for that date,\n“Federal Funds Upper Target” - represents the upper bound for that date,\n“Federal Funds Lower Target” - represents the lower bound for that date,\n“Effective Federal Funds Rate” - represents effective rate for that date,\n“Real GDP (Percent Change)” - represents percentage change in GDP on that date,\n“Unemployment Rate” - tells us information about unemployment on that date,\n“Inflation Rate” - gives the inflation rate on that date.\nEach row gives us information about macro economic indicators related to effective federal rates for a given date.\n(3) What is the unit of observation? In other words, what does each case mean in this data?\nAnswer: Here each row is a case/observation. Unit of observation is a date and corresponding macro economic indicators related to federal rates.\nGenerating a date column:\nNotice that the year, month, and day are three different columns. We will first have to use a string function called “str_c()” from the “stringr” library to combine these three columns into one “date” column. Please revise the following commands\n\nfed_rates &lt;- fed_rates_org %&gt;%\n  mutate(date = str_c(Year, Month, Day, sep=\"-\"))\n\nMove the new created “date” column to the beginning as the first column of the data.\n\n#Type your code here\nfed_rates &lt;- fed_rates %&gt;%\n  select(date, everything())\n\nWhat is the data type of the new “date” column?\n\n#Type your code here\nclass(fed_rates$date)\n\n[1] \"character\"\n\n\nTransform the “date” column to a &lt;date&gt; data.\n\n#Type your code here\nfed_rates &lt;- fed_rates %&gt;%\n  mutate(date = as.Date(date, format = \"%Y-%m-%d\"))\n\nclass(fed_rates$date)\n\n[1] \"Date\"\n\n\nConduct following statistics:\n\n#Type your code here\n# Dates of highest unemployment ratee\nfed_rates %&gt;%\n  summarise(\"Dates of highest unemployment rate\" = date[!is.na(`Unemployment Rate`) & `Unemployment Rate` == max(`Unemployment Rate`, na.rm = TRUE)] )\n\n\n\n  \n\n\n# Dates of lowest unemployment rate\nfed_rates %&gt;%\n  summarise(\"Dates of lowest unemployment rate\" = date[!is.na(`Unemployment Rate`) & `Unemployment Rate` == min(`Unemployment Rate`, na.rm = TRUE)] )\n\n\n\n  \n\n\n\n(1) On which date is the highest unemployment rate? and the lowest?\nAnswer:\nThere are multiple dates with highest and lowest unemployment rates, listed them below:\nDates of highest unemployment rate = 1982-11-01, 1982-12-01\nDates of lowest unemployment rate = 1968-09-01, 1968-10-01, 1968-11-01, 1968-12-01, 1969-01-01, 1969-02-01, 1969-03-01, 1969-04-01, 1969-05-01\n(2) (Optional) Which decade has the highest average unemployment rate?\nAnswer: The decade 1974-1983 has the highest unemployment rate.\n\nHere is a template for you to create a decade column to allow you to group the data by decade. You can use it for the optional question in Challenge#1:\n\nfed_rates &lt;- fed_rates |&gt;\n  mutate(Decade = cut(Year, breaks = seq(1953, 2027, by = 10), labels = format(seq(1954, 2017, by = 10), format = \"%Y\")))\n\nfed_rates %&gt;%\n  group_by(Decade) %&gt;%\n  summarise(\"average_unemployment_rate\" = mean(`Unemployment Rate`, na.rm = TRUE)) %&gt;%\n  arrange(desc(average_unemployment_rate), .by_group=TRUE)\n\n\n\n  \n\n\n#using different definition of decade\n#fed_rates %&gt;%\n#  mutate(Decade = Year-Year%%10) %&gt;%\n#  group_by(Decade) %&gt;%\n#  summarise(avg_unemployment_rate = mean(`Unemployment Rate`, na.rm = TRUE)) %&gt;%\n#  arrange(desc(avg_unemployment_rate), .by_group=TRUE)\n\n\n\n##Note: the cut() a baseR function that we don't generally use. Basically, it allows us divides the range of Year into intervals and codes the values in Year according to which interval (1954 and 2017) they fall; the break argument specifies how we segmate the sequence of Year (by a decade)"
  },
  {
    "objectID": "challenge_4_Fall23.html",
    "href": "challenge_4_Fall23.html",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "",
    "text": "Make sure you change the author’s name in the above YAML header."
  },
  {
    "objectID": "challenge_4_Fall23.html#setup",
    "href": "challenge_4_Fall23.html#setup",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Setup",
    "text": "Setup\nIf you have not installed the following packages, please install them before loading them.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(haven) #for loading other datafiles (SAS, STATA, SPSS, etc.)\nlibrary(stringr) # if you have not installed this package, please install it.\nlibrary(ggplot2) # if you have not installed this package, please install it.\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "challenge_4_Fall23.html#challenge-overview",
    "href": "challenge_4_Fall23.html#challenge-overview",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nIn this challenge, we will practice with the data we worked on in the previous challenges and the data you choose to do some simple data visualizations using the ggplot2 package.\nThere will be coding components and writing components. Please read the instructions for each part and complete your challenges."
  },
  {
    "objectID": "challenge_4_Fall23.html#datasets",
    "href": "challenge_4_Fall23.html#datasets",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Datasets",
    "text": "Datasets\n\nPart 1 the ESS_Polity Data (created in Challenge#3) ⭐⭐\nPart 2: the Australia Data⭐⭐\nPart 3: see Part 3. Practice plotting with a dataset of your choice (25% of the total grade). For online platforms of free data, see Appendix: sources for data to be used in Part 3.\n\nFind the _data folder, then read the datasets using the correct R command."
  },
  {
    "objectID": "challenge_4_Fall23.html#part-1.-univariate-and-multivariate-graphs-45-of-the-total-grade",
    "href": "challenge_4_Fall23.html#part-1.-univariate-and-multivariate-graphs-45-of-the-total-grade",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Part 1. Univariate and Multivariate Graphs (45% of the total grade)",
    "text": "Part 1. Univariate and Multivariate Graphs (45% of the total grade)\nWe have been working with these two data in the previous three challenges. Suppose we have a research project that studies European citizens’ social behaviors and public opinions, and we are interested in how the countries that respondents live in influence their behavior and opinion. In this challenge, let’s work with the combined dataset ESS_Polity and create some visualizations.\n\nRead the combined data you created last time. (2.5%)\n\n\n#type of your code/command here.\nESS_Polity = read.csv(\"./challenge4_data/ESS_Polity.csv\")\n\n\nSuppose we are interested in the central tendencies and distributions of the following variables. At the individual level: age, male, edu, income_10, and vote. At the country level: democ.\n(1) Recode the “vote” column: if the value is 1, recode it as 1; if the value is 2, recode it as 0; if the value is 3, recode it as NA. Make sure to include a sanity check for the recoded data. (2.5%)\n\n#Recoding vote column with sanity checks\ntypeof(ESS_Polity$vote)\n\n[1] \"integer\"\n\nunique(ESS_Polity$vote)\n\n[1]  3  2  1 NA\n\n#table(ESS_Polity$vote, useNA=\"always\")\n\nESS_Polity &lt;- ESS_Polity %&gt;%\n  mutate(vote = case_when(\n        vote==1 ~ 1,\n        vote==2 ~ 0,\n        vote==3 ~ NA,\n        TRUE ~ NA\n      ))\n\ntypeof(ESS_Polity$vote)\n\n[1] \"double\"\n\nunique(ESS_Polity$vote)\n\n[1] NA  0  1\n\n#table(ESS_Polity$vote, useNA=\"always\")\n\n(2) For each of the five variables (age, edu, income_10, vote, and democ), please choose an appropriate type of univariate graph to plot the central tendencies and distribution of the variables. Explain why you choose this type of graph to present a particular variable (for example: “For example, I use a histogram to plot age because it is a continuous numeric variable”). (25%)\n(Note: You should use at least two types of univariate graphs covered in the lecture.)\nage:\nA histogram is a suitable choice when dealing with continuous or discrete numeric variables, which is the case with age. The histogram also allows you to see the shape of the distribution, whether it’s skewed, normal, bimodal, etc helping us in understanding the distribution of data.\n\n#type of your code/command here.\n#age\n#typeof(ESS_Polity$age)\n#unique(ESS_Polity$age)\n#table(ESS_Polity$age, useNA=\"always\")\nggplot(data = ESS_Polity, aes(x = age)) +\n  geom_histogram(binwidth = 2, color=\"black\", na.rm = TRUE) +\n  labs(title = \"Age Distribution\", x = \"Age\", y = \"Frequency\")\n\n\n\n\nedu:\nA bar plot or count plot is suitable for displaying the frequency or count of each education level. It is an effective way to visualize categorical or ordinal data with a limited number of unique values. Here, we are not displaying NA values, since they can be a bit of distraction, we can include NA values in graph too by just passing whole of the data into “data” argument instead of filtering.\n\n#edu\n#typeof(ESS_Polity$edu)\n#unique(ESS_Polity$edu)\n#table(ESS_Polity$edu, useNA=\"always\")\nggplot(data = filter(ESS_Polity, !is.na(edu)), aes(x = factor(edu))) +\n  geom_bar(color = \"black\", na.rm = TRUE) +\n  labs(title = \"Education Level Distribution\", x = \"Education Level\", y = \"Count\")\n\n\n\n\nincome:\nA bar plot or count plot is suitable for displaying the frequency or count of each income level. It is an effective way to visualize categorical or ordinal data with a limited number of unique values. Here, we are not displaying NA values, since they can be a bit of distraction, we can include NA values in graph too by just passing whole of the data into “data” argument instead of filtering.\n\n#income\n#typeof(ESS_Polity$income_10)\n#unique(ESS_Polity$income_10)\n#table(ESS_Polity$income_10, useNA=\"always\")\nggplot(data = filter(ESS_Polity, !is.na(income_10)), aes(x = factor(income_10))) +\n  geom_bar(color = \"black\",  na.rm = TRUE) +\n  labs(title = \"Income Level Distribution\", x = \"Income Level\", y = \"Count\")\n\n\n\n\nvote:\nA bar plot or count plot is suitable for displaying the frequency or count of vote category. It is an effective way to visualize categorical or ordinal data with a limited number of unique values. This allows us to easily see the distribution of vote and their frequencies, helping us in understanding the data. Here too, we are not displaying NA values.\n\n#vote\n#typeof(ESS_Polity$vote)\n#unique(ESS_Polity$vote)\n#table(ESS_Polity$vote, useNA=\"always\")\nggplot(data = filter(ESS_Polity, !is.na(vote)), aes(x = factor(vote))) +\n  geom_bar(color = \"black\") +\n  labs(title = \"Vote Distribution\", x = \"Vote value\", y = \"Count\")\n\n\n\n\ndemoc:\nA bar plot or count plot is suitable for displaying the frequency or count of each democracy level. It is an effective way to visualize categorical or ordinal data with a limited number of unique values. This allows us to easily see the distribution of democracy levels and their frequencies, helping us in understanding the data. Here too, we are not displaying NA values.\n\n#democ\n#typeof(ESS_Polity$democ)\n#unique(ESS_Polity$democ)\n#table(ESS_Polity$democ, useNA=\"always\")\nggplot(data = filter(ESS_Polity, !is.na(democ)), aes(x = factor(democ))) +\n  geom_bar(color = \"black\") +\n  labs(title = \"Democracy level Distribution\", x = \"Democracy level\", y = \"Count\")\n\n\n\n\nSuppose we want to test two hypotheses on the relationships of two pairs of variables. Please use the appropriate type of graphs we learned to visualize these two pairs of variables. Briefly describe the graph you plot, and answer: Does the graph we create from the data support the hypothesis?\n(1) Hypothesis#1: The more years of education (edu) a person completed, the higher income (income_10) they earn. (7.5%)\nAnswer:\nThe graphs we created below, supports the hypothesis 1.\nBelow we plotted 2 graphs, one is a scatter plot and the other is a grouped bar plot. From the scatter plot we cannot clearly see if there is a relation between the education level and income level because the data is spread over across all the education and income levels, which makes it hard to see a pattern. But the grouped bar plot shows clear patterns between education level and income level. People with higher education level tend to have more income and people with less education level tend to have less income.\n\n\n#education and income relationship\nggplot(data = filter(ESS_Polity, !is.na(edu), !is.na(income_10)), aes(x = edu, y = income_10)) +\n  geom_point() +\n  labs(title = \"Scatterplot of Education vs. Income\", x = \"Education level\", y = \"Income Level\")\n\n\n\nggplot(filter(ESS_Polity, !is.na(edu), !is.na(income_10)), aes(x=factor(edu), fill=factor(income_10))) +\n  geom_bar(position=\"dodge\") +\n  labs(title=\"Education vs income\", x=\"Education level\", y=\"Count\")\n\n\n\n\n(2) Hypothesis#2: There is a gender disparity (male) in voting behavior (vote). (Either men are more likely to vote, or women are more likely to vote). (7.5%)\nAnswer:\nThe graphs we created below, does not support the hypothesis 2.\nBelow we plotted the grouped bar plot to see if there is any gender disparity in voting behavior. But from my understanding there is not much gender disparity here. Even though there is some difference in the numbers, it is not significant enough to be considered a pattern. So we can say that there is no gender disparity in voting behavior.\n\nggplot(filter(ESS_Polity, !is.na(vote), !is.na(male)), aes(x=factor(vote), fill=factor(male))) +\n  geom_bar(position=\"dodge\") +\n  labs(title=\"Gender Disparity in voting study\", x=\"Voting Behavior\", y=\"Count\")"
  },
  {
    "objectID": "challenge_4_Fall23.html#part-2.-comparing-between-partial-and-whole-and-among-groups-30-of-the-total-grade",
    "href": "challenge_4_Fall23.html#part-2.-comparing-between-partial-and-whole-and-among-groups-30-of-the-total-grade",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Part 2. Comparing between Partial and Whole, and among Groups (30% of the total grade)",
    "text": "Part 2. Comparing between Partial and Whole, and among Groups (30% of the total grade)\nIn this part, we will use the clean version of the Australian public opinion poll on Same-Sex Marriage to generate graphs and plots. You may need to do the data transformation or mutation needed to help graphing.\n\nRead in data. (2.5%)\n\n#type of your code/command here.\naus_data_org &lt;- read.csv(\"challenge4_data/australian_data.csv\")\n\nUse a barplot to graph the Australian data based on their responses: yes, no, illegible, and no response. The y-axis should be the count of responses, and each response should be represented by one individual bar (so there should be four bars). (7.5%)\n(you can use either geom_bar() or geom_col())\n\naus_data &lt;- aus_data_org %&gt;%\n  gather(key = \"Response_type\", value = \"Count\", Yes, No, Illegible, `No.Response`)\nhead(aus_data)\n\n\n\n  \n\n\ngrouped_data &lt;- aus_data %&gt;%\n  group_by(`Response_type`) %&gt;%\n  summarise(Total_count = sum(Count))\n\nggplot(data = grouped_data, aes(x = Response_type, y = Total_count)) +\n  geom_bar(stat = \"identity\", color=\"black\") +\n  labs(title = \"Vote distribution\", x = \"Response type\", y = \"Count\")\n\n\n\n\nThe previous graph only shows the difference in amount. Let’s create a stacked-to-100% barplot to show the proportion of each of the four responses (by % of the total response). (7.5%)\n(you can use either geom_bar() or geom_col())\n\ngrouped_percentage_data &lt;- aus_data %&gt;%\n  group_by(`Response_type`) %&gt;%\n  summarise(Total_count = sum(Count)) %&gt;%\n  mutate(proportion = Total_count/sum(Total_count))\n\nggplot(data = grouped_percentage_data, aes(x = \"\", y = proportion, fill = Response_type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Total Response proportion graph\", x = \"\", y = \"Percentage\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 100)) \n\n\n\n\nLet’s see if there’s a relationship between Division and Response - that is, are certain divisions more likely to respond one way compared to other divisions? Again, we will use barplot(s) to present the visualization. (12.5%)\n(you can use either geom_bar() or geom_col())\nAnswer:\nWe plotted 2 plots below, one is normal stacked bar plot and the other is stacked-to-100%. We cannot see clear relationship between Division and Response from first graph because the divisions have different population count and so cant see the proportions clearly. That is why I created the second graph which shows proportion of responses for each division.\nObservations:\n1. For all the divisions the proportion of Illegible votes is very less.\n\n\nFor all the divisions except for Northern Territory division, the proportion of Yes votes is slightly more than other response types.\n\nOnly in Northern Territory division, the proportion of “no response” is higher than other type of responses.\n\n\n#type of your code/command here.\ndivision_grouped_data &lt;- aus_data_org %&gt;%\n  group_by(Division) %&gt;%\n  summarise(Yes=sum(Yes), No=sum(No),Illegible=sum(Illegible),`No.Response`=sum(`No.Response`)) %&gt;%\n  gather(key = \"Response_type\", value = \"Count\", Yes, No, Illegible, `No.Response`)\n\nggplot(data = division_grouped_data, aes(x = Division, y = Count, fill = Response_type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Division Responses\", x = \"Division\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 335, hjust = 0))\n\n\n\ndivision_grouped_percentage_data &lt;- division_grouped_data %&gt;%\n  group_by(Division) %&gt;%\n  mutate(percentage = 100*Count/sum(Count))\n\nggplot(data = division_grouped_percentage_data, aes(x = Division, y = percentage, fill = Response_type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Divison Response proportion graph\", x = \"\", y = \"Percentage\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  theme(axis.text.x = element_text(angle = 335, hjust = 0))"
  },
  {
    "objectID": "challenge_4_Fall23.html#part-3.-practice-plotting-with-a-dataset-of-your-choice-25-of-the-total-grade",
    "href": "challenge_4_Fall23.html#part-3.-practice-plotting-with-a-dataset-of-your-choice-25-of-the-total-grade",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Part 3. Practice plotting with a dataset of your choice (25% of the total grade)",
    "text": "Part 3. Practice plotting with a dataset of your choice (25% of the total grade)\nIn this part, you will choose data of your interests for graphing and plotting. This data can be tidy/ready-to-be-used or raw data that needs cleaning. If the data is very large (for example, more than 20 columns), you should definitely subset the data by selecting less than 10 variables of your interests to avoid taking too much room in your R memory.\n\nInclude a link to the data page (this page should include the introduction or description and the link to download this dataset). (2%)\nAnswer:\nI am working on the “Customer Shopping Trends Dataset” from kaggle. The link to the data page is given below:\nhttps://www.kaggle.com/datasets/iamsouravbanerjee/customer-shopping-trends-dataset?select=shopping_trends.csv\nRead the data you choose and briefly answer the following questions. (Optional: you may need to subset, clean, and transform the data if necessary). (8%)\n\n#reading the data and cleaning the data.\nshopping_data &lt;- read.csv(\"challenge4_data/shopping_trends.csv\")\nshopping_data &lt;- shopping_data %&gt;%\n  mutate(Purchase.Item.Category = Category) %&gt;%\n  select(Customer.ID, Age, Gender, Purchase.Item.Category, Season, Previous.Purchases, Review.Rating, Subscription.Status, Preferred.Payment.Method, Frequency.of.Purchases) \n\nhead(shopping_data)\n\n\n\n  \n\n\ncolnames(shopping_data)\n\n [1] \"Customer.ID\"              \"Age\"                     \n [3] \"Gender\"                   \"Purchase.Item.Category\"  \n [5] \"Season\"                   \"Previous.Purchases\"      \n [7] \"Review.Rating\"            \"Subscription.Status\"     \n [9] \"Preferred.Payment.Method\" \"Frequency.of.Purchases\"  \n\n\n(1) what is the structure (dimension) of the data;\nAnswer:\nThe dimension of the data is 3900x10. 3900 rows and 10 columns.\n(2) what is the unit of observation?\nAnswer:\nHere the unit of observation is a particular customer. Each row is a case which gives information about the customer’s shopping information.\n(3) what does each column mean in this data?\nAnswer:\nThe columns are:\nCustomer.ID: Unique identifier for each customer,\nAge - Age of the customer\nGender - Gender of the customer (Male/Female)\nPurchase.Item.Category: Category of the item purchased\nSeason: Season during which the purchase was made\nPrevious.Purchases: The total count of transactions concluded by the customer at the store, excluding the ongoing transaction\nReview.Rating: Rating given by the customer for the purchased item\nSubscription.Status: Indicates if the customer has a subscription (Yes/No)\nPreferred.Payment.Method: Customer’s most preferred payment method\nFrequency.of.Purchases: Frequency at which the customer makes purchases (e.g., Weekly, Fortnightly, Monthly)\nChoose two columns/variables of your interests. Plot one univariate graph for each of the variables. (5%)\nAnswer:\nI am plotting the univariate graphs for Age and Subscription.Status below:\n\n#Age distribution plot\n#unique(shopping_data$Age)\n#table(shopping_data$Age)\nggplot(data = shopping_data, aes(x = Age)) +\n  geom_histogram(binwidth = 10, color=\"black\", na.rm = TRUE) +\n  labs(title = \"Age Distribution\", x = \"Age\", y = \"Frequency\")\n\n\n\n#Subscription.Status distribution plot\n#unique(shopping_data$Subscription.Status)\nggplot(data = shopping_data, aes(x = factor(Subscription.Status))) +\n  geom_bar(color = \"black\") +\n  labs(title = \"Subscription Distribution\", x = \"Subscription Status\", y = \"Count\")\n\n\n\n\n\n\n\nChoose a pair of variables you suspect or hypothesize may be correlated and a graph (scatter plot or barplot) using them. Based on the visual evidence, do you see any potential correlation between the two variables (10%)\nAnswer:\nHypothesis: Is there any relationship between subscription status and purchase frequency.\nObservation: From the plot we can observe that there is no significant pattern to say that there is a relation ship between Subscription.Status and Frequency.of.Purchases.\n\n#type of your code/command here.\nggplot(shopping_data, aes(x=factor(Subscription.Status), fill=factor(Frequency.of.Purchases))) +\n  geom_bar(position=\"dodge\") +\n  labs(title=\"Subscription status vs purchase frequency\", x=\"Subscription Status\", y=\"Count\")"
  },
  {
    "objectID": "challenge_4_Fall23.html#appendix-sources-for-data-to-be-used-in-part-3",
    "href": "challenge_4_Fall23.html#appendix-sources-for-data-to-be-used-in-part-3",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Appendix: sources for data to be used in Part 3",
    "text": "Appendix: sources for data to be used in Part 3\nHere are some online sources and popular Online Dataset Hub:\n\nMany US governments (usually at the federal and state levels),  bureaus, and departments have open data archives on their websites, allowing the public to access, download, and use them. Just use Google to search for them.\n\n\n\nThe Harvard Dataverse Repository is a free data repository open to all researchers from any discipline, inside and outside the Harvard community, where you can share, archive, cite, access, and explore research data. Each individual Dataverse collection is a customizable collection of datasets (or a virtual repository) for organizing, managing, and showcasing datasets.\n\n\n\nInter-university Consortium for Political and Social Research (ICPSR) of the University of Michigan-Ann Arbor provides leadership and training in data access, curation, and methods of analysis for the social science research community. \n\n\n\nUN: https://data.un.org/\n\n\n\nOECD Data:  economic and development data of the most developed countries in the world.\n\n\n\nThe upper five sources are mainly for social science data; there is another very big community and open data archives for machine-learning and data science: Kaggle."
  },
  {
    "objectID": "challenge_5_Fall23.html",
    "href": "challenge_5_Fall23.html",
    "title": "Challenge_5: Visualizing Time and Space",
    "section": "",
    "text": "Make sure you change the author’s name in the above YAML header."
  },
  {
    "objectID": "challenge_5_Fall23.html#setup",
    "href": "challenge_5_Fall23.html#setup",
    "title": "Challenge_5: Visualizing Time and Space",
    "section": "Setup",
    "text": "Setup\nIf you have not installed the following packages, please install them before loading them.\n\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(readxl)\nlibrary(haven) #for loading other datafiles (SAS, STATA, SPSS, etc.)\n\n#for plotting time\nlibrary(ggplot2) # if you have not installed this package, please install it.\nlibrary(lubridate)\n\n\n#for plotting space\nlibrary(sp)\nlibrary(sf)\nlibrary(maps)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(gganimate)\nlibrary(gifski)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "challenge_5_Fall23.html#challenge-overview",
    "href": "challenge_5_Fall23.html#challenge-overview",
    "title": "Challenge_5: Visualizing Time and Space",
    "section": "Challenge Overview",
    "text": "Challenge Overview\n\n\n\nHalloween2023\n\n\nIn this challenge, we will practice the visualization skills learned in the class with two datasets to capture the temporal and spatial patterns of supernatural phenomena.\nThere will be coding components and writing components. Please read the instructions for each part and complete your challenges."
  },
  {
    "objectID": "challenge_5_Fall23.html#datasets",
    "href": "challenge_5_Fall23.html#datasets",
    "title": "Challenge_5: Visualizing Time and Space",
    "section": "Datasets",
    "text": "Datasets\n\nPart 1. The UFO Sightings Data (50%)\nPart 2. The Haunted Places Data (50%)\n\nFind the _data folder, then read the datasets using the correct R command."
  },
  {
    "objectID": "challenge_5_Fall23.html#part-1.-the-ufo-sightings-data-50",
    "href": "challenge_5_Fall23.html#part-1.-the-ufo-sightings-data-50",
    "title": "Challenge_5: Visualizing Time and Space",
    "section": "Part 1. The UFO Sightings Data (50%)",
    "text": "Part 1. The UFO Sightings Data (50%)\nThis data contains over 80,000 reports of UFO sightings over the last century in six major countries (and other places). You can learn more about this data by checking: https://www.kaggle.com/datasets/NUFORC/ufo-sightings.\n\nRead and Describe the Data (10%)\nWhat is the dimension of the data? What do the columns mean? What is the unit of observation?\nAnswer:\nThe dimension of the data is 88875x11.\n\nThe following are the columns:\ndatetime - the date on which the ufo was sighted\ncity - the city in which the ufo was sighted\nstate- the state in which the ufo was sighted\ncountry - the country in which the ufo was sighted\nshape - the shape of the ufo\nduration (seconds) - the duration in seconds for which the ufo was sighted\nduration (hours/min) - the duration in minutes/hours for which the ufo was sighted\ncomments - comments regarding the sighting\ndate posted - the date on which the sighting was posted\nlatitude - latitude location of ufo spotted\nlongitude - longitude location of ufo spotted\n\nHere, the unit of observation is a UFO sighting instance. Each row is a case giving information about a ufo sighting e.g., location, duration etc.\n\n\n#type of your code/command here.\nufo_data &lt;- read_csv(\"./challenge5_data/complete_UFO.csv\", show_col_types = FALSE)\n\nufo_data &lt;- ufo_data %&gt;%\n  select(-`...12`)\n\ndim(ufo_data)\n\n[1] 88875    11\n\nhead(ufo_data)\n\n\n\n  \n\n\ncolnames(ufo_data)\n\n [1] \"datetime\"             \"city\"                 \"state\"               \n [4] \"country\"              \"shape\"                \"duration (seconds)\"  \n [7] \"duration (hours/min)\" \"comments\"             \"date posted\"         \n[10] \"latitude\"             \"longitude\"           \n\n\n\n\n\nPlease plot a temporal/time-series graph to present the following patterns. You may need to subset or mutate the data for graphing.\n(1) the total number of UFO sighting reports over the years (date_break = year). (15%)\n\n#change the datetime column to Date class\nufo_data$datetime &lt;- as.Date(ufo_data$datetime, format = \"%m/%d/%Y\")\n\n# Create a time series for the total number of sightings over the years\nufo_year_data &lt;- ufo_data %&gt;%\n  group_by(year = lubridate::year(datetime)) %&gt;%\n  summarize(total_reports = n())\n\n# Create the plot -  we can change the number of years shown by changing the scale\nggplot(ufo_year_data, aes(x = year, y = total_reports)) +\n  geom_line() +\n  labs(title = \"Total Number of UFO Sighting Reports Over the Years\", x = \"Year\", y = \"Total Reports\") +\n  scale_x_continuous(breaks = seq(min(ufo_year_data$year), max(ufo_year_data$year)+10, by = 10 )) \n\n\n\n  #theme(axis.text.x = element_text(angle = -90, hjust = 1))\n\n(2) the total number of UFO sighting reports by months between 2010-01-01 and 2014-01-01. (15%)\n\n#UFO sighting reports by months between 2010-01-01 and 2014-01-01.\nufo_month_data &lt;- ufo_data %&gt;%\n  filter(datetime &gt;= as.Date('2010-01-01') & datetime &lt;= as.Date('2014-01-01')) %&gt;%\n  group_by(monthyear = floor_date(datetime, \"month\")) %&gt;%\n  summarize(total_reports = n())\n\n\n# Plotting the monthly data for the specified period\nggplot(ufo_month_data, aes(x = monthyear, y = total_reports)) +\n  geom_line() +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b %Y\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +\n      labs(title = \"Total Number of UFO Sighting Reports by Month (2010-01-01 to 2014-01-01)\", x = \"Year-Month\", y = \"Total Reports\")\n\n\n\n\nPlease write a paragraph describing the patterns you find on the two graphs above. (10%)\nAnswer:\nPlot1:\nWe can observe from the first plot that the number of UFO sightings were very low from 1906 to 1992 but after that there is significant exponential rise in number of UFO sightings. In 2014, we can observe a drop again which might be due to incomplete data (the data has information only till May 2014). The rise in number maybe due to awareness among the people or technological advancements helping to record the reports better.\n\nPlot2:\nWhen we plot the UFO sighting report count between 2010-01-01 and 2014-01-01, we can observe an interesting pattern. During a given year, number of reported sightings are low at the start and end of the year (During Jan, Feb, Nov, Dec etc) but there is a significant rise in reports during the middle of the year (mainly in the months of June and July).\n\n(Optional) Use gganimte and gifsky packages to plot gifs of the above time-series plots. You can refer to codes and commands in the Week#8 demo file.\n\n#yearly plot\nufo_yearly_plot_animated &lt;- ggplot(ufo_year_data, aes(x = year, y = total_reports)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Total Number of UFO Sighting Reports Over the Years\", x = \"Year\", y = \"Total Reports\") +\n  scale_x_continuous(breaks = seq(min(ufo_year_data$year), max(ufo_year_data$year)+10, by = 10 )) +\n  transition_reveal(year)\n\nanimate(ufo_yearly_plot_animated, duration = 20, fps = 10, width = 500, height = 500, renderer = gifski_renderer())\n\n\n\nanim_save(\"UFOYearlyPlot.gif\")\n\n(Optional) Suppose we are interested in describing the country variations in types and numbers of UFO sighting reports. How can we use bar plots to visualize the difference across countries (using the country column, you can recode the empty cells with “others”)? Note that you may need to do the data transformation or mutation needed to help graphing.\n\n#type of your code/command here.\nufo_data_mod &lt;- ufo_data %&gt;%\n    mutate(country = ifelse(is.na(country), \"others\", country))\n\n\nufo_country_totals &lt;- ufo_data_mod %&gt;%\n  group_by(country) %&gt;%\n  summarise(total_reports = n())\nggplot(data = ufo_country_totals, aes(x = factor(country), y = total_reports)) +\n      geom_bar(stat = \"identity\", color = \"black\", na.rm = TRUE) +\n      labs(title = \"Country wise UFO reports\", x = \"Country\", y = \"Number of reports\")\n\n\n\nggplot(ufo_data_mod, aes(x=factor(country), fill=factor(shape))) +\n      geom_bar(position=\"dodge\") +\n      labs(title=\"Shape variations for Countries\", x=\"Country\", y=\"Count\")"
  },
  {
    "objectID": "challenge_5_Fall23.html#part-2.-the-haunted-places-data-50",
    "href": "challenge_5_Fall23.html#part-2.-the-haunted-places-data-50",
    "title": "Challenge_5: Visualizing Time and Space",
    "section": "Part 2. The Haunted Places Data (50%)",
    "text": "Part 2. The Haunted Places Data (50%)\nThis data contains the reports of haunted places in the United States. The dataset was compiled by Tim Renner, using The Shadowlands Haunted Places Index, and shared on data.world. You can learn more about this data by checking: https://www.kaggle.com/datasets/sujaykapadnis/haunted-places\n\nRead and Describe the Data (10%) (Note: you don’t need to remove the empty rows after the first row; read_csv() should automatically remove them).\nWhat is the dimension of the data? What do the columns mean? What is the unit of observation?\nAnswer:\nThe dimension of the data is 10992x10. There are 10992 rows and 10 columns.\n\nThe columns present in the data are the following:\ncity - The city where the place is located.\ncountry - The country where the place is located (always “United States”)\ndescription - A text description of the place.\nlocation - A title for the haunted place.\nstate - The US state where the place is located.\nstate_abbrev - The two-letter abbreviation for the state.\nlongitude - Longitude of the place.\nlatitude - Latitude of the place.\ncity_longitude - Longitude of the city center.\ncity_latitude - Latitude of the city center.\nThe unit of observation here is a haunted place. Each row is a case and gives us information about the haunted place e.g., location.\n\n\n#reading data\nhaunted_places &lt;- read_csv(\"./challenge5_data/haunted_places.csv\", show_col_types = FALSE)\n\ndim(haunted_places)\n\n[1] 10992    10\n\nhead(haunted_places)\n\n\n\n  \n\n\ncolnames(haunted_places)\n\n [1] \"city\"           \"country\"        \"description\"    \"location\"      \n [5] \"state\"          \"state_abbrev\"   \"longitude\"      \"latitude\"      \n [9] \"city_longitude\" \"city_latitude\" \n\n\nPlot a USA map with states boundaries. There are multiple ways of plotting this map. (15%)\nYou can use the geom_polygon() (which requires a data of spatial coordinates for plotting a polygon), or you can use geom_sf() (which requires a shapefile of the US). Please refer to the examples in the Week#8 demo file.\n\nstates_sf&lt;-st_read(\"./challenge5_data/States_shapefile.shp\")\n\nReading layer `States_shapefile' from data source \n  `E:\\UMASS_Acads\\Sem3\\601DACCS\\DACSS601\\challenge5_data\\States_shapefile.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 51 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -178.2176 ymin: 18.92179 xmax: -66.96927 ymax: 71.40624\nGeodetic CRS:  WGS 84\n\nusa_map&lt;-ggplot()+\n  geom_sf(data = states_sf)+\n  coord_sf(xlim = c(-180, -65), ylim = c(20, 70))+\n  theme_light() +\n  labs(title = \"USA Map with State Boundaries\")\n\nprint(usa_map)\n\n\n\n\nPlot the haunted places using the city_longtitude and city_latitude with geom_point() on the USA map generated above. (15%)\n\n#type of your code/command here.\nhaunted_places_map&lt;-ggplot()+\n  geom_sf(data = states_sf)+\n  geom_point(data = haunted_places, aes(x = city_longitude, y = city_latitude), pch = 19, size=0.00005, na.rm = TRUE )+\n  coord_sf(xlim = c(-180, -65), ylim = c(20, 70)) +\n  theme_light() +\n  labs(title = \"Location of haunted places on USA Map\")\n\nprint(haunted_places_map)\n\n\n\n\nPlease write a paragraph describing the pattern of the haunted spaces you find on the map above. (10%)\nAnswer:\nWe can observe from the above plot that the Eastern part of USA has more haunted places than the Western plot. States wise, California has the highest number of haunted places and Alaska has the lowest number of haunted places. And there seems to be more density of haunted places near the coastal regions.\n\n#type of your code/command here.\nhaunted_places %&gt;%\n  group_by(state) %&gt;%\n  summarise(Number_of_places = n()) %&gt;%\n  arrange(desc(Number_of_places))"
  },
  {
    "objectID": "challenge_6_Fall23_template.html",
    "href": "challenge_6_Fall23_template.html",
    "title": "Challenge_6: Basic Principles of Data-Driven Story-telling",
    "section": "",
    "text": "Make sure you change the author’s name in the above YAML header."
  },
  {
    "objectID": "challenge_6_Fall23_template.html#challenge-overview",
    "href": "challenge_6_Fall23_template.html#challenge-overview",
    "title": "Challenge_6: Basic Principles of Data-Driven Story-telling",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nIn this challenge, we will mainly apply the principles we learned from Jane Miller’s book in practice. You will review multiple examples of data description and presentation intext, table/number, and charts/figures. Please read the instructions for each part and complete your challenges.\nFor all the screenshots, images, or tables mentioned in the questions, please see the Challenge_6_Fall23.html file. You don’t need to include any of these items in your rendered challenge file."
  },
  {
    "objectID": "challenge_6_Fall23_template.html#part-1.-the-ufo-sightings-data-50",
    "href": "challenge_6_Fall23_template.html#part-1.-the-ufo-sightings-data-50",
    "title": "Challenge_6: Basic Principles of Data-Driven Story-telling",
    "section": "Part 1. Simple Applications",
    "text": "Part 1. Simple Applications\n\nRecall Jane Miller’s Ws mentioned in Chapter 2. One of the W’s (Who, What, When, and Where, Why) is missing from each of the following table descriptions. Rewrite each sentence to include that information.\n\n\n“Germany did the best at the 2002 Winter Olympics, with 35 medals, compared to 34 for the United States, 24 for Norway, and 17 for Canada.”\nAnswer:\n“Germany did the best at the 2002 Winter Olympics, with 35 medals, compared to 34 medals for the United States, 24 medals for Norway, and 17 medals for Canada.”\n“Gold, silver, and bronze medals each accounted for about one-third of the medal total.”\nAnswer:\n“Gold, silver, and bronze medals each accounted for about one-third of the medal total at the 2002 Winter Olympics.”\n“At the 2002 Winter Olympics, the United States won more medals than all other countries, followed by Canada, Germany, and Norway.”\nAnswer:\n“At the 2002 Winter Olympics, the United States won more Bronze medals than all other countries, followed by Canada, Germany, and Norway.\n\nFor each of the following situations, specify whether you would use prose of text, a table of numbers, or a particular type of chart/figure. Explain why you chose this way to present the data.\n\nStatistics on five types of air pollutants in the 10 largest US cities for a government report\nAnswer:\nI would use a table for statistics on five types of air pollutants in the 10 largest US cities for a government report. A table can provide numerical data for each city and each type of air pollutant in a concise manner making it easy for comparison and analysis.\n\nTrends in the value of three stock market indices over one year for a web page.\nAnswer:\nI would use a line chart for showing trends in the value of three stock market indices over one year for a web page. We can have time on the x-axis and value of a stock index on y-axis. A line graph like this more intuitive and can help us in understanding the patterns of stock market value over time and also help us comparing the indices with one another easily.\n\nNotification to other employees in your corporation of a change in shipping fees\nAnswer:\nI would use a prose or simple text to notify other employees in my corporation regarding the change in shipping fees. The information here is simple and straightforward, so using complex modes is unnecessary. Just a simple clear prose can do the job and ensure clarity at the recipient’s end.\n\nDistribution of voter preferences for grade-level composition of a new middle school (grades 5–8, grades 6–8, or grades 6–9) for a presentation at a local school board meeting.\nAnswer:\nA bar chart would be a good option to show the distribution of voter preferences for grade-level composition of a new middle school. This visual representation of data will be easily understandable by the audience and can help them in making better decisions using this data.\n\nNational estimates of the number of uninsured among part-time and full-time workers for an introductory section of an article analyzing effects of employment on insurance coverage in New York City.\nAnswer:\nSince this is part of an article and the data is not too complex, we can present this data in the form of a prose. The text can be descriptive so that this is easily understandable by the audience and can be written in such a way that it grasps the attention of the audience.\n\nRead the sentences below. What additional information would someone need in order to answer the associated question?\n\n“Brand X costs twice as much as Brand Q. Can I afford Brand X?”\nAnswer:\nHere we need to know the available funds or the budget of the person buying the product. And we need to know the price of either one of the Brand X or Brand Q products.\n\n“My uncle is 6’6” tall? Will he fit in my new car?“\nAnswer:\nHere we need to know the interior dimensions of the car, particularly the headroom and how important being comfortable (how much they can compromise) is for their uncle.\n\n“New Diet Limelite has 25% fewer calories than Diet Fizzjuice. How much faster will I lose weight on Diet Limelite?”\nAnswer:\nHere mainly we need to know the daily calorie intake of the person and how much calories they burn on a daily basis. The amount of calories prensent in either of the Diet’s is not that important as long as we know the daily intake. With this we can get a rough idea of how faster they can loose weight but still it depends on many other biological factors.\n\n“It has been above 25 degrees every day. We’re really having a warm month, aren’t we?”\nAnswer:\nHere we need more information about the scale on which the current temperature is measured and also the general temperature patterns of the area over the past few months/years. With this information we can tell it they were having a warm month or not.\n\n\nIndicate whether each of the following sentences correctly reflects table 4B. If not, rewrite the sentence so that it is correct. Check both the correctness and completeness of these sentences.\nNote: According to Wikipedia, ” In political science, voter turnout is the participation rate (often defined as those who cast a ballot) of a given election. This is typically the percentage of registered, eligible, or all voting-age people.”\n\n\nBetween 1964 and 1996, there was a steady decline in voter participation.\nAnswer:\nIncorrect.\nBetween 1964 and 1996, there was a steady decline in registered voter participation except for 1992.\n\nVoter turnout was better in 1996 (63.4%) than in 1964 (61.9%).\nAnswer:\nIncorrect.\nRegistered Voter turnout was better in 1964 (95.8%) compared to 1996 (63.4%).\n\nThe majority of all registered voters participated in the 1964 US presidential election.\nAnswer:\nCorrect, but the below is more complete (gives more information) than above statement.\nThe majority (95.8%) of all registered voters participated in the 1964 US presidential election.\n\nThe best year for voter turnout was 1992, with 104,600 people voting.\nAnswer:\nIncorrect.\nThe best year for Total Votes casted was 1992, with 104,600 people voting.\n\nA higher percentage of the voting-age population was registered to vote in 1996 than in 1964.\nAnswer:\nCorrect, but the below is more complete (gives more information) than above statement.\nA higher percentage of the voting-age population was registered to vote in 1996 (74.4%) than in 1964 (64.6%).\n\nIdentify terms that need to be defined or restated for a non-technical audience without much knowledge about the topic or statistical method. You don’t need to explain these terms (you don’t need to know any of the statistical methods mentioned). Just identify them.\na. “According to the latest study based on the VDem Dataset, the average Rule of Law score is statistically higher in democratic countries than non-democratic countries based on a t-test (p = 0.01).”\nAnswer:\nThe following terms need to be defined or restated for a non-technical audience:\nVDem, Rule of Law score, t-test, p\nb. ” According to the logistic regression results in the screenshot below, we can see a positive correlation between household income and the vote choice for G.W. Bush, with a positive coefficient (log-odd = 0.33). “\nAnswer:\nThe following terms need to be defined or restated for a non-technical audience:\nlogistic regression, correlation, coefficient, log-odd\nThe terms from the image:\nglm, bionmial, logit, coef.est, coef.se\n\n\n\nPart 2. Practical Applications\nSuppose you work as a data analyst in the music-producing industry. One day, you get a data report that studies the popularity of different genres of music. The following scatterplot is presented to you. There is no text description for either this table or the data.\n\n(1) What information can you describe or summarize based on the current graph?\nAnswer:\nIt is given that the graph is about popularity of different genres of music. From this we can interpret that one of the axis is popularity and the other is genre. The y-axis says pop so it must be popularity and the x-axis gives the genre code (different genres are given different numerical numbers). This means each data point of the scatter plot might be a song from a particular genre.\n\n\n\nThe genre that is encoded as 3 has popularity value all over the place, meaning some songs of this genre are popular and some are not, some are very popular etc.\n\nSimilarly, the songs that correspond to the genres that are encoded as 5, 6, 9, 10 and 12 are mostly popular (pop &gt; 50).\n\nWe can make similar arguments about other genres too.\n\nBut one thing we have to keep in mind is, we don’t know how many songs are present for each genre for a partular popularity. Since the counts are not given we cannot make strong arguments about the genres whose popularity is spread across y-axis. For example - for genre encoded as 11, the points corresponding to low popularity maybe are just 2 songs, and all the other songs happen to have the same popularity represented by the top 2 dots on the plot or the reverse. The scatter plot here fail to convey that information, making it hard to make general arguments about a genre.\n\n\n\n(2) Thinking of the principles we learned in the week of visualization customization and Jane Miller’s principles, What additional information (Please describe at least three things (at least one thing that is NOT about graph customization, such as title, color, label, etc.) that you consider adding this table so that it can convey meaningful information.\nAnswer:\n\n\nLabels - The x-axis label should be appropriate - not “variable_1” and y-axis label should convey what it is, “pop” is not a great abbreviation for popularity.\n\nTitle - can help us in getting an idea what this graph is about.\n\nCount - Here we don’t have information about the number of songs that are being plotted for a particular genre. Since the points in the scatter plot can overlap on each other, we don’t have definitive information making it hard to understand the data better.\n\n\n\n(For your reference, this is the original source of the dataset: https://www.kaggle.com/datasets/leonardopena/top-spotify-songs-from-20102019-by-year)\nTwo articles on scientific studies talk about the “risks.” Please read the titles and quotes from these two articles and answer the following two questions.\n\n(1) Given the information provided, in which case is there a greater “risk”: the Pancreatic Cancer case or the Diabetes case?\nAnswer:\nHere we cannot exactly compare both the risks since we are not measuring them on the same scale. One is based on number of sodas consumed per week but does not tell for how long it is consumed, while the other is based on 1 soda per day over a decade. Since the time frames are different and not complete, we cannot compare them.\n(2) Think of Jane Miller’s principles. What additional information would you need to know to compare the “risk” in the two cases?\nAnswer:"
  }
]